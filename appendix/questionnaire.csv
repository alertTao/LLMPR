description,commit messages,linked issue titles,original,T5,BART,Llama3,DeepSeek-V2,BlueLM
" this adds visit* methods for the astbuilder to convert an antlr tree into a ql tree. i've only scoped this to stateless expressions (no sequence, join, pipes, ancestry). there are a few cases of eql are just shorthand for now, unless we add more optimal/direct support within ql: x in (a, b, c, ...) -> x == a or x == b or x == c or ... x == ""some*wildcard*expr*"" -> wildcard(x, ""some*wildcard*expr*"") functions get turned into unresolvedfunction. i think at some point, we'll need to add ql support for these or have these functions be eql only with a custom registry. i haven't dived much into that yet to see how this works. i'm assuming that this will be done in a separate follow up pr (created issue #51556 for new functions) related issues #49589 #49997 ", eql: add astbuilder visitors  eql: add tests for wildcards and sets  eql: fix licensing ,,add astbuilder to convert to ql tree,add astbuilder visitors eql,add visit* methods to astbuilder,adds visit* methods for the astbuilder to convert an antlr tree into a ql tree, Add visit* methods for ASTBuilder to convert Antlr tree to QL tree,eql: add astbuilder visitors
" i did correct some translation, and also i added link to let people visualize the code and test other attributes on it. i will edit this article when i prepare some new things in it, so it become more understandable. i have read freecodecamp's contribution guidelines. my pull request has a descriptive title (not a vague title like update index.md) my pull request targets the master branch of freecodecamp. none of my changes are plagiarized from another source without proper attribution. my article does not contain shortened urls or affiliate links. if your pull request closes a github issue, replace the xxxxx below with the issue number. "," total change  i changed the definition and the explanation, and make every possibilies of every logical type, and added some pictures of truth table to make the article and the sentences more understandable.  total change  article edited  i did correct some translation, and also i added link to let people visualize the code and test other attributes on it.  i will edit this article when i prepare some new things in it, so it become more understandable. ",,added arabic translation to html/table article,"i did correct some translation, and added link to let people visualize the code and test other attributes on it",edited article and added link to it,improve article on logical operators with truth tables, improved logical operators article with visual aids and translations,update article on boolean algebra (translated)
 this fixes #1121. #2999 should be merged first. , modified server ssl certs to allow multiple pairs and force_client_auth flag  fixed tests , change the node ssl server credentials api so that a server can have multiple key/cert pairs. ,allow node server credentials to have multiple key/cert pairs,ssl server credentials api to allow multiple pairs,change server ssl certs api so that a server can have multiple key/cert pairs,bugfix - modified server ssl certs to allow multiple pairs and force_client_auth flag, node ssl server credentials api update for multiple key/cert pairs,allow multiple ssl key/cert pairs for node ssl credentials
 deleted typography.js which unnecessarily complicates the example for styled-components moved createglobalstyle to a separate file with good practise , delete typography.js which shouldn't be in the styled-components example  moved globalstyle to external file with good practise ,,delete typography.js and move createglobalstyle to separate file for styled-components example,delete typography.js which shouldn't be in styled-components example,removed typography.js which unnecessarily complicates styled-components example,fix styled-components example with good practise, delete typography.js and move globalstyle to external file in styled-components example,simplify styled-components example
" it is fine to resolve the types of exported classes in es6: export class c { } var c = new c() but not for commonjs exported classes: module.exports.c = class { } var c = new c() // should error jsdoc type aliases are an exception since they are always exported implicitly, not by [property] assignment to module.exports or exports. fixes #24492 ", fix resolution of exported types in commonjs  it is fine to resolve the types of exported classes in es6:  js  export class c {  }  var c = new c()    but not for commonjs exported classes:  js  module.exports.c = class {  }  var c = new c() // should error    fixes #24492  all jsdoc type aliases are available locally in commonjs modules ," in js, crash on incorrect type reference of a module.exports property assignment ",fix exported type resolution in commonjs,fix resolution of exported types in commonjs,fix resolution of exported types in commonjs,improved handling of structurally identical classes in union types and instanceof expressions, Fix Type Resolution for CommonJS Exported Classes,improved handling of structurally identical classes
 i added wgsl support and keep glsl until finish nodematerial wgsl code generate. updated the example webgpu_compute and scale parameter behavior.  this contribution is funded by google via igalia. , webgpu: hybrid language  webgpu_compute: update of glsl -> wgsl  highlight point color  update screenshot ,,wgsl support and webgpu_compute example updated,add wgsl support and keep glsl until finish nodematerial wgsl code generate,update of glsl -> wgsl example,webgpu and nodematerial: add wgsl support, WGSL Support and GLSL Removal for WebGPU and NodeMaterial,webgpu glsl to wgsl conversion
 fixes #14522 (this demonstrates that mocking is possible and shows how). fix a bug in test.callasyncduplexstreamingcall (reverse order of generic args). show how to create a fake client stub show how to test server-side impl classes that implement the generated server methods. , add tests demonstrating how to mock client stubs  demonstrate testability of server-side impl classes , c# how to mock generated client base? ,add c# tests that demonstrate how to unit test grpc code with test doubles.,show how to mock server-side impl classes,show how to mock client stubs,add tests demonstrating testability of server-side impl classes, demonstrate mocking and testing in c# grpc,add tests demonstrating how to mock client stubs and test server-side impl classes
" this continues the port of the java comparison tool to python. changes to the blockstore and a small change in main.cpp are in separate commits. this includes and runs both the ""inexpensive"" and ""barely expensive"" tests. the ""barely expensive"" test (which consists of a few re-orgs) take a minute or two on my machine. java test is here: ", tests: rework blockstore to avoid re-serialization.  continuing port of java comptool  catch exceptions from non-canonical encoding and print only to log ,,continuing port of java comparison tool,continue port of java comparison tool to python,continue port of java comparison tool,Simplify ray.init() by invoking ray start internally, Continue porting Java comparison tool to Python,port of java comptool continued
" as discussed in dotnet/aspnetcore#15351, the behavior of the ""defaults"" methods could be clarified a bit. i had a few minutes so i took a stab at it. internal render: kestrel web server implementation in asp.net core configure certificate authentication in asp.net core ", update kestrel.md  update certauth.md  update kestrel.md  update kestrel.md  update certauth.md ,,add a clarifying note about kestrel's configureendpointdefaults and configurehttpdefaults methods,configure certificate authentication in asp.net core,update kestrel web server implementation in asp.net core,clarify defaults methods in asp.net core," Clarify ""defaults"" methods behavior in Kestrel and certificate authentication docs",clarify defaults methods in kestrel and certauth
 this pull request add the support of can iso-tp address extensions in the socket module by enhancing getsockaddrarg and getsockname. note to core developpers : i believe this pull request should be backported back to python 3.3 where socketcan support have been added. , added support for can_isotp protocol  added unit tests for can isotp  updated documentation for iso-tp protocol ,,bpo-30987 - support for iso-tp protocol in socketcan,add support for can isotp address extensions,add support for can iso-tp address extensions,add support for can iso-tp address extensions, Add CAN ISO-TP Address Extensions Support in Socket Module,add can_isotp address extensions to socket module
" closes #34966 tests added / passed (all tests passed except those which were auto-cancelled for taking too long) ensure all linting tests pass, see here for how to run them whatsnew entry allows default resolvers to be used with input resolvers in pandas.dataframe.eval. previously cases such as this would break: import pandas as pd df = pd.dataframe({'a':[0,1,2], 'b':[7,8,9]}) d = {'c': 5} df.eval('a+b*c', resolvers=[d]) with pandas.core.computation.ops.undefinedvariableerror: name 'a' is not defined as explained in #34966. following this update, the above works as expected. added a test similar to the above: tests.frame.test_query_eval.testdataframeeval.test_eval_resolvers_combined "," bug: default+input resolvers in df.eval, gh34966  allow default resolvers to be used with input resolvers in dataframe.eval. resolves gh34966  add whatsnew entry ", doc: confusing 'resolvers' kwarg documentation for dataframe.query() ,"allow use of both default+input resolvers in df.eval, gh34966",fix default+input resolvers in df.eval,"allow default resolvers in df.eval, gh34966",fix default+input resolvers in df.eval, default and input resolvers compatibility in DataFrame.eval,allow default resolvers to be used with input resolvers in dataframe.eval
" currently plugin just resolves url() paths relative to the main sass entry file. resolve-url-loader adds the ability to use paths relative to the file where the url() is being used. by default if not configured in the gatsby-plugin-sass, the loader is not being used, but you can activate it and also configure resolve-url-loader plugin with the options it provides. usage has been also documented in the readme.md of the gatsby-plugin-sass fixes #7776, #6438 ", using resolve-url-loader if configured in the options  documenting how to use resolve-url-plugin plugin  fixing small typo , how to use resolve-url-loader with gatsby-plugin-sass in v2? ,add option to enable resolve-url-loader,fix a small typo in gatsby-plugin-sass plugin,use resolve-url-loader if not configured in the options,feat(gatsby-plugin-sass): add resolve-url-loader support, Add resolve-url-loader support to gatsby-plugin-sass,use resolve-url-loader if configured
 for #6869. , refactor mergedencryptcolumnsmergedresulttest  use static import with mockito.mock  use static import with mockito.returns_deep_stubs  use static import with mockito.when  remove useless mock on mergedencryptcolumnsmergedresulttest  add mockeddatasource  use mockeddatasource instead of h2 data source in spring namespace test cases  use mockeddatasource instead of h2 data source in spring namespace test cases  move encrypt spring namespace to encrypt module  refactor encryptspringnamespacetest  move master-slave spring namespace test to current module  remove useless abstractspringjunittest  move shadow spring namespace test to correct module  remove useless xml  move spring sharding test into correct module  revise springnamespacetest  update encryptspringnamespacetest  update masterslavespringnamespacetest  update shadowspringnamespacetest ,,move spring namespace' test cases to correct modules,refactor mergedencryptcolumnsmergedresulttest,move spring namespace tests to encrypt module,refactor spring namespace tests for #6869, Refactor Spring Namespace Tests with MockedDataSource,move encrypt and master-slave spring namespace to correct modules
" there are 2 proposed fixups in discussions in #23280 which i have not implemented: an overhaul to return types and an option type for the two *chainstate functions: #23280 (comment) the change reintroduces stringy return types and is quite involved. it could be discussed in a separate pr. passing in the unix time to verifychainstate instead of a callback to get the time: #23280 (comment) i'm not sure it matters much whether it's a callback or just the actual unix time. also, i think verifydb can take quite a while, and i don't want to impose that the function have to ""run quickly"" in order to have it be correct. if reviewers feel strongly about either of the two fixups listed above, please feel free to open a pr based on mine and i'll close this one! ", node/chainstate: use max_future_block_time  docs: make loadchainstate comment more accurate  style-only: rename *chainstate return values  init: use clang-tidy named args syntax ,,"post-""chainstate loading sequence coalescence"" fixups",reintroduce stringy return types and option type for the two *chainstate functions,make loadchainstate comment more accurate,fix return types and options for chainstate functions, node/chainstate updates and documentation improvements,node/chainstate: use max_future_block_time
" add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). the package does not already provide its own types, or cannot have its .d.ts files generated via --declaration create it with dts-gen --dt, not by basing it on an existing project. represents shape of module/library correctly tslint.json should be present and it shouldn't have any additional or disabling of rules. just content as { ""extends"": ""dtslint/dt.json"" }. if for reason the some rule need to be disabled, disable it for that line using // tslint:disable-next-line [rulename]  and not for whole package so that the need for disabling can be reviewed. tsconfig.json should have noimplicitany, noimplicitthis, strictnullchecks, and strictfunctiontypes set to true. see react redux | custom hooks and the source code (each file contains a createsomething function which is typed in this package. the current typing for the createetc functions reflect basic return and argument types, but does not reflect the relation between the argument and return type. this is especially important (in fact, without the relation, the type is useless) because the return type cannot be used, in general, when the argument is non-trivial (i.e. the argument is not {}). the depreciation of the typeduseselectorhook is because the new types will obsolete that type. include tests for your changes if you are making substantial changes, consider adding a tslint.json containing { ""extends"": ""dtslint/dt.json"" }. if for reason the any rule need to be disabled, disable it for that line using // tslint:disable-next-line [rulename] and not for whole package so that the need for disabling can be reviewed. add it to notneededpackages.json. ", update index.d.ts  update react-redux-tests.tsx  update index.d.ts ,,improved factory function types and deprecated typeduseselectorhook.,update index.d.ts and update index.d.ts,update types for react redux,update react-redux typings to reflect createSomething function relationships, Update Type Definitions for React Redux to Reflect Custom Hooks and Function Relationships,update index.d.ts and tests for react-redux
" .s16 sign extends on loads, but it seems to be the same as .u16 on stores. since our ir doesn't support primitives for integers smaller than 32 bits, implement the logic manually. in the future we might want to handle 16 and 8 bit operations at its respective level on the ir and use 32 bits operations on the decompilation step when the target api doesn't support smaller integers. while we are at it, abstract the code to avoid repeating logic. used by pc builder simulator used by doom 2016 ", shader/memory: implement ldl.s16 and lds.s16  shader/memory: move unaligned load/store to functions  shader/memory: implement unaligned ldl.s16 and lds.s16  shader/memory: implement stl.s16 and sts.s16 ,,"implement ldl.s16, lds.s16, stl.s16 and sts.s16",implement stl.s16 and sts.s16 on loads,implement ldl.s16 and lds.s18,"implement ldl.s16, lds.s16, stl.s16 and sts.s16"," implement ldl.s16, lds.s16, stl.s16, and sts.s16 for 16-bit operations",implement 16 bit integer load/store
 changes to provide api in objective-c to the values and names for enums. ," sync to thomasvl/protobuf head  added new api to gpbenumdescriptor to enable introspection of enum values.  refactored implementation so that this contains a minimum of added code.  clarified comments regarding behavior in the presence of the alias_allowed option.  added unit tests for new functionality and for the alias case.  oops, fixed spelling error.  more commentsmithing.  addressed tom's comment on the pull request.  noticed two more instances of fixes in the earlier commit. ",,add ability to introspect list of enum values,added api to gpbenumdescriptor to enable introspection of enum values,add objective-c api to enum values,improve enum api in objective-c, Add Objective-C API for Enum Introspection,improved Objective-C API for GPBEnumDescriptor
" why invert colors? when using flameshot in dark environments on a regular basis it would be very useful to invert the colors in the capture, making the screenshot white(ish) background and dark text. thus, enhancing the usability of those screenshots in reports and prints without having to use additional tools or workarounds. also, it's been a feature request for quite a while (#689). how does it work? basically, i copied the savetool and made an inverttool, that inverses all pixels before saving the screenshot. the actual consists of four lines: qpixmap inverted = context.selectedscreenshotarea(); qimage img = inverted.toimage(); img.invertpixels(); inverted.convertfromimage(img); however, it turned out that flameshot requires to update quite a few places in order to add a tool. without much of a documentation i tried to stay close to how the savetool is setup. before merging i'd advise that someone with a way better understanding of this project looks at my modifications in order to spot potentially missing or even unnecessary code for this new tool to be integrated. example see the above mentioned issue for a video demo. here's a screenshot of the newly added button. usage after choosing the area and drawing markers, text, arrows etc. - use ctrl+i or the invert button to save the screenshot with inverted colors. notes this isn't a perfect solution. people might want to invert colors while still drawing stuff or see a review of the inversion before saving the capture. additionally, more filters could be added maybe (comment by @mmahmoudian). however, my qt knowledge isn't by far good enough to be able to help with those things. from my point of view, this invert tool is a good solution for the time being and could still be extended at a later point. alright, that's it - let me know if i missed something or should update anything. ", add invert tool to build commands  add icons for invert tool  integrate invert tool  add shortcut for invert tool  add invert tool  add translation for invert tool  run clang-format ,,add a tool to take an inverted screenshot,add invert tool to build commands,invert colors when using flameshot,Implement Invert Colors Tool for Flameshot, Add Invert Tool for Flameshot to Invert Colors in Screenshots,add invert tool
" add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). provide a url to documentation or source code which provides context for the suggested changes:  if you are making substantial changes, consider adding a tslint.json containing { ""extends"": ""dtslint/dt.json"" }. if for reason the any rule need to be disabled, disable it for that line using // tslint:disable-next-line [rulename] and not for whole package so that the need for disabling can be reviewed. ", added missing $filter overload  added missing $filter overload that allows multiple strings to translate and returns an object  merge remote-tracking branch 'upstream/master'  make visitor.id optional  update tests ,,pendo.io - make visitor.id optional in initialize(),add missing $filter overload,make visitor.id optional and update tests,Added missing $filter overload and made visitor.id optional, make visitor.id optional and update tests,add location type arg for query params to route definition v3
" a suggestion for improving docs. i spent a while trying to figure out where additional configmaps are uploaded to the halyard pod on a k8s spinnaker deployment so they could be used in config scripts, and this one line would've helped. i realize spinnaker is deployed on multiple kinds of infra, not just k8s, which is why i didn't mention a halyard pod, but maybe there's a better way to make this note generic. dco signed "," move the api_host variable to deck.yml, where it belongs  bump chart version for gate fix  merge remote-tracking branch 'upstream/master'  noting where additional config maps are written to disk may be helpful to beginners ",,add note to spinnaker doc about destination path for additionalconfigmaps,fix halyard pod configmaps for k8s,add note about where additional configmaps are uploaded to disk,improve documentation for halyard configmap upload locations, improve documentation on configmap location for halyard,add note about additional configmaps uploaded to halyard pod
 remove the code that is not useful in the loop , remove spaces before and after the properties  update mixall.java  remove spaces before and after the properties  remove spaces before and after the properties  remove spaces before and after the properties ,,[rocketmq-226]remove the code that is not useful in the loop,remove the code that is not useful in the loop,remove spaces before and after the properties,fix whitespace issues in properties files," Refactor properties files: fix trailing spaces, missing escapes, and formatting issues",remove spaces before and after the properties
" #243 added a single-endpoint configuration option for an elasticache replication group that used dns polling to determine when a failover had occurred. this had the drawback of requiring the application to ensure the jvm did not cache dns lookups. additionally, it left the read slave(s) of the replication group unused; only the master node would ever be queried. this pr adds an elasticachereplicationgroupserversconfig (such a long class name!) for the specification of all nodes in the replication group (as auto-discovery is unsupported in redis elasticache). a new elasticachereplicationgroupconnectionmanager then uses info replication to determine the role of each node and polls each node at a configurable interval (default: 1000ms - same as cluster polling) to determine if the role has changed. example code: public static void main(string[] args) { config config = new config(); config.useelasticachereplicationgroupservers().addnodeaddress(args); redisson r = redisson.create(config); r.flushdb(); ratomiclong al = r.getatomiclong(""testlong""); while (true) { try { system.out.println(system.currenttimemillis() +"":""+ al.addandget(1)); } catch (throwable t) { system.out.println(""exception: "" + t); t.printstacktrace(system.out); } try { thread.sleep(1000); } catch (interruptedexception ignored) {} } } output (foo-002 is promoted while program runs): java -jar test.jar foo-001.cache.amazonaws.com:6379 foo-002.cache.amonaws.com:6379 1441997395811:1 1441997396819:2 1441997397822:3 ... 1441997422911:31 exception: org.redisson.client.redisexception: readonly you can't write against a read only slave.. channel: [id: 0xddd6cc97, /10.0.x.x:42009 => foo-001.cache.amazonaws.com/10.0.y.y:6379] command: commanddata [promise=defaultpromise@644b9548(incomplete), command=rediscommand [name=incrby, subname=null], params=[testlong, 1], codec=org.redisson.client.codec.stringcodec@469956b0] ... 1441997435035:32 1441997436041:33 ... note exception still occurs for the command that was pending against foo-001 when failover happened, but will succeed on retry after the failover has been detected. "," elasticache replication group server configuration  uses ""info replication"" to determine which node is master and which are  slaves.  no need to invoke slavedown(). also some non-functional cleanup ",,master / slave configuration for elasticache replication group,add a single-endpoint configuration option for elasticache replication group,"use ""info replication"" to determine which nodes are master and which are slave",fix removestashindex of objectmap, Add Elasticache Replication Group Server Configuration and Connection Manager,add support for specifying all nodes in an elasticache replication group
" this pr proposes to migrate the test_compatible_versions unit test from snapshots to a blocks.log based system.  this removes any dependency on the post-conditions of the methods used to construct blockchains in our tester framework. instead of building a ""new"" chain in a way that was intended to be deterministic but rarely was in practice, the new process will load replay a small blocks.log file to reconstruct the head state of the chain with the latest code.   it will then systematically check that this state is also recoverable from v2, v3 and v4 snapshots of that same chain. this pr also restores the full integrity checksum based check. the blocks.log was created using v1.8.14 so as to have minimal requirements on the reading process.  if the minimum compatible blocks.log version exceeds the version produced by v1.8.14 then this blocks.log may need to be migrated to a newer version. the snapshots were created with the following versions based on this blocks.log: v2 : v1.8.14 tag v3 : v2.0.7 tag v4 :  26e4100 commit (aka head before the new snapshot was added) the unit test now takes an additional program option: --generate-snapshot-log which will attempt to deterministically recreate the blocks.log file prior to running the test.  this will not overwrite the repo version automatically.  there is no guarantee that the resulting chain matches the existing log but this may be useful in the future when a snapshot bump cleans out old versions and we want to start fresh the pre-existing --save-snapshot still works and will be useful as a forward compatible way of generating the future snapshots for this test.  when a new version is created, the author would need to add it to the list of tested versions, run the unit-test with --save-snapshot then copy the file(s) from <build>/unitttests/snapshosts/snap_v*.{bin,json}.gz to the working tree. ", migrate to log based construction  refactor snapshot test to build state from a blocks.log file instead of a set of tester methods; create v4 snapshots in prep for farming older versions  created a blocks.log compatible with 1.8.x  created v2 snapshots from v1.8.14 tag  created v3 snapshots from v2.0.7 tag  re-created v4 snapshots with the blocks.log that was 1.8.x compatible ,,change to log based snapshot test,migrate to log based construction,migrate test_compatible_versions unit test to blocks.log based system,Migrate test_compatible_versions unit test from snapshots to blocks.log based system, migrate test_compatible_versions from snapshots to blocks.log based system,migrate test_compatible_versions to use blocks.log for snapshots
" this updates the release notes for the breaking changes to -usehd option and the getinfo rpc. also, bumps the manpages to current master. ", doc: bump manpages to 0.15.99  doc: update release notes for 0.16.0 ,,update release notes and manpages for 0.16,update release notes for 0.16.0,update release notes for 0.16.0,update release notes for 0.16.0 and bump manpages, Update Release Notes for 0.16.0 and Bump Manpages to 0.15.99,bump manpages to 0.15.99 and update release notes for 0.16.0
" since the endianness of test dataset is little-endian, conversions are required on big-endian machines.  related issue is also written in #1730 this pr fixes small part of the issue, and we need to convert more to pass all onnx tests by pytest. however, this enables all of onnx-mlir test to pass. the test cases are written in test code i would like to hear your comments or suggestions about this approach. ", added big-endian support by converting little-endian data in numpy_helper  convert byte ordering only in test which uses little-endian data (removed previous modification to numpy_helper.py)  removed unnecessary modification for onnf test  removed unnecessary blank ,,convert endianness of test dataset to pass tests of onnx-mlir on big-endian machines,added big-endian support by converting little-endian data in numpy_helper,add big-endian support to numpy_helper,fix big-endian support for onnx-mlir tests, added big-endian support for onnx-mlir tests,added big-endian support by converting little-endian data in numpy_helper
" removed and renamed several files that do not exist or have been renamed, but the project files had never been updated.  this was causing an issue building with visual studio 2015/2017, with a ""project is out of date"" warning every time the program is run from within the ide. "," [libcocos2d.vcxproj, libcocos2d.vcxproj.filters] removed entries for non-existent files that were causing ""build is out of date"" issues in visual studio  [project.pbxproj] removed entries for non-existent file  [libcocos2d.vcxproj.filters] renamed ccstencilstatemanager.h extension to .hpp.  [libcocos2d.vcxproj, libcocos2d.vcxproj.filters] removed entry for file ccdownloaderimpl.h which does not exist. ",,vs and xcode project files have non-existent file references,removed and renamed files that do not exist or have been updated,"removed and renamed files that were causing ""build is out of date"" issues",fix project files for visual studio 2015/2017," Fix ""build is out of date"" issues in Visual Studio 2015/2017",remove and rename several files that do not exist
 for #11709 , rename clusterpersistrepositoryconfiguration  refactor gov spring namespace  refactor standalone spring name space  refactor cluster spring name space  remove useless codes  move cluster repository.xsd  move standalone repository.xsd  rename test cases ,,refactor gov spring namespace to cluster,refactor clusterpersistrepositoryconfiguration gov spring namespace,refactor gov spring namespace,rename and refactor spring namespace configurations, Refactor Spring Namespace for Governance and Standalone Configurations,refactor spring namespace for governance
 fix instructions mentioned in #6181 goes with r2r pr radare/radare2-regressions#627 , enhance sub op support for thumb arch  generate correct instructions up to 0x100  improve support for add instruction for thumb arch ,,fix add and sub for arm thumb fix #6181,improve support for thumb arch,improve sub op support for thumb arch,fix thumb assembly issues #6181, fix thumb assembly instructions #6181,update thumb arch disassembler
 summary added the op batch_normalization in the numpy backend. related issues pr overview added the function in the numpy backend added the numpy implementation in the docs added tests. this pr requires new unit tests [y/n] (make sure tests are included) this pr requires to update the documentation [y/n] (make sure the docs are up-to-date) this pr is backwards compatible [y/n] this pr changes the current api [y/n] (all api changes need to be approved by fchollet) , finished the functions.  started doing the test function.  added the batch_normalization operation to the numpy backend. ,,added batch_normalization in the numpy backend.,added the batch_normalization function in the numpy backend,added batch_normalization operation to the numpy backend,added batch normalization to numpy backend, added batch_normalization operation to numpy backend with tests and docs,added the op batch_normalization in the numpy backend
 this closes probtorch#87. math checked and approved by @fritzo cc: @apaszke ," init f distribution  rename f to fishersnedecor, add tests and docs  remove other samplers, fix shape error and add new kl for dirichlet  update entropy function in fisher-snedecor  add comment regarding f-distribution entropy test  remove entropy and revert changes in kl.py ", implement f-distribution ,implementation of the fisher-snedecor distribution,"rename f to fishersnedecor, add tests and docs",implement f distribution in fishersnedecor,implementation of the f-distribution, implement f-distribution and related updates,[DONE] implement f-distribution
" fixes #19074 this change disables image lazy-loading when both of the following are true: a image is being rendered following a client-side page transition the image has been previously loaded during this session. before this change, all images with lazy-loading enabled have a visible flicker during client-side page transitions, even though they're already loaded. with this change, there's are two performance risks: there's a chance that some offscreen images will have lazy-loading disabled unnecessarily because they were previously loaded. i think the performance hit here is pretty negligible and the situation is unlikely to come up very often. there's a chance a different-sized version of the image will be selected by the browser, but lazy-loading will be disabled anyway. this seems even more unlikely to me, and anyway the performance hit from a stray un-lazy-loaded image (on a client-side transition) is very minor. in both cases, i think the performance risk is outweighed by the ux improvement of getting rid of the image flicker on page transition. ", don't lazy-load already-loaded image in csr  fixes 19074  scope change to client-side only , [next/image] repeated images rerender on every page switch. ,don't lazy-load already-loaded image in client-side transition,don't lazy-load already-loaded image in csr,don't lazy-load already loaded images in csr,fix image lazy-loading flicker on client-side page transitions, fix image flicker during client-side page transitions by disabling lazy-loading for previously loaded images,[next/image] disable lazy-loading for already-loaded images on client-side transitions
" fixes  #66304 r? @gilescope shows both the actual as well as the expected panic value when a test with should_panic(expected=...) fails. this makes should_panic more consistent with assert_eq. i am not sure whether printing the any::type_id() is useful, is there something better that we could print for non-string panic values? ", print a more useful error message on should_panic mismatch  replace some asserts with assert_eq for better error readability , better error messages for #[should_panic] tests with expected strings ,more useful test error messages on should_panic(expected=...) mismatch,print more useful error messages on should_panic mismatch,better error messages for #[should_panic] tests with expected strings,better error messages for #[should_panic] tests with expected strings, improve error messages for #[should_panic] tests with expected strings (fixes #66304),print actual and expected panic value when should_panic fails
" signature help for tagged templates this pr enables signature help support in the language service for tagged template strings. the idea is that a tagged template is a bit of an implicit function invocation. whenever a user requests signature help (or it is triggered by the consuming environment), we should figure out what function a tag refers to, which overload should be chosen, and which parameter is currently being fed. substitution-argument correspondence when a template expression is tagged, each substitution expression corresponds to an argument in invoking that tag. specifically, the nth substitution expression in a tagged template expression is the (n+1)th argument in an invocation. templatestringsarray argument whenever the cursor lies within a template literal (whether a no-substitution literal, or a fragment of a template expression), the parameter in question is the first parameter of the tag in question. this is because each template literal in a tagged template corresponds to an array element in the cooked/raw array object specified in es6. this means that as the cursor moves through a template expression, parameters will not be highlighted from left-to-right consistently. "," initial signature help work for tagged templates.  stylistic changes/comment fixups.  got sig help working in the template head.  got sig help working in tagged no-sub templates.  refactored code, adjusted for residing out of bounds of the template.  fixed isunclosedtemplateliteral to account for new possible inputs.  conflicts:  src/services/signaturehelp.ts  fixed template head offsetting.  tests for signature help on tagged templates with no overloads.  added tests for overloads.  fixed broken test. ",,tagged template signature help support in language service,signature help for tagged templates,signature help for tagged templates,add signature help for tagged templates, enable signature help support for tagged template strings,enable signature help for tagged templates
" implements wait and retry logic on ray.connect() this takes care of the case where the server is not yet ready, but does not yet cover the case where the server drops mid-connection (still to come). first half of #13353 i've run scripts/format.sh to lint the changes in this pr. ", [ray_client]: wait until connection ready  change-id: ie443be60c33ab7d6da406b3dcaa57fbb7ba57dd6  lint  change-id: i30f8e870bbd5f8859a9f11ae244e210f077cedd0 ,,wait for ready and retry on ray.connect(),wait and retry logic on ray.connect(),wait until server is ready,fix wait and retry logic on ray.connect(), Implement wait and retry logic for ray.connect(),add wait and retry logic to ray.connect()
 adds getstringview as a convenience function to get string_view from a string returning an empty string_view on null pointer. ," added missing endtable() call to verifyobject()  verifyobject called verifytablestart() but not endtable(). this made verifier::verifycomplexity() increase depth_ with each table, not with the depth of tables.    merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  added check to verifyalignment    add getstringview (convenience function to get string_view from a string returning an empty string_view on null pointer) like getstring, getcstring ",,"add getstringview like getstring, getcstring",add getstringview as a convenience function,add getstringview and getcstring to verifyalignment,add getstringview as a convenience function to get string_view from a string returning an empty string_view on null pointer, add getstringview for string_view convenience,add getstringview as a convenience function to get string_view from a string returning an empty string_view on null pointer
" while streaming from obs on an unstable connection, i have found instances where obs will hang until it is force quit. during some testing with arut/nginx-rtmp-module proxying connections i found that when the obs that was streaming to nginx-rtmp-module had network issues it would cause the obs to hang and never recover. i eventually identified that the server was sending a netstream.publish.badname status message that was never handled. more information from adobe about that error can be found on their site. i would like obs to not hang when there are network glitches. i tested this on macos by making a build on my macbook pro and did the following: from the nginx config: application stream { live on; publish_notify on; play_restart on; } steps: turn off wifi and connect the macbook pro via ethernet start obs via the command line start a custom stream to an nginx-rtmp server disconnect the ethernet plug it back in 10 seconds later i checked the logs and status bar to see if the system reconnected. it would not reconnect when using obs in master but would reconnect when using this branch. bug fix (non-breaking change which fixes an issue) my code has been run through clang-format. i have read the contributing document. ", obs-outputs: handle rtmp netstream.publish.badname response  adobe media server and nginx-rtmp can return this status response to a  publisher if the key is already being used to publish.  obs-outputs: log unhandled rtmp status responses  rtmp status responses that are not handled are currently silently ignored  making it difficult to identify issues. ,,handle netstream.publish.badname from rtmp server,handle obs-outputs on unstable connections,don't hang on unstable connections,fixes for rtmp connection stability, handle rtmp netstream.publish.badname response and log unhandled statuses,fix rtmp netstream.publish.badname hang
" currently, the decisions regarding which translog generation files to delete are hard coded in the interaction between the internalengine and the translog classes. this pr extracts it to a dedicated class called translogdeletionpolicy, for two main reasons: simplicity - the code is easier to read and understand (no more two phase commit on the translog, the engine can just commit and the translog will respond) preparing for future plans to extend the logic we need - i.e., retain multiple lucene commit and also introduce a size based retention logic, allowing people to always keep a certain amount of translog files around. the latter is useful to increase the chance of an ops based recovery. ", wip  wip  extract interfaces  java doc tweak  wip  translog tests compile  translog tests pass  remove ontranslogrollover as it's not needed for now  simplification and removal of future stuff  update java docs  tell combineddeletionpolicy of open mode so it can be smarter.  introducing indexcommitref  # conflicts:  #	core/src/main/java/org/elasticsearch/index/engine/internalengine.java  #	core/src/main/java/org/elasticsearch/index/shard/indexshard.java  #	core/src/test/java/org/elasticsearch/index/engine/internalenginetests.java  a little test to test translog min reference advance  some java docs ,,introducing a translog deletion policy,extract translog files to a dedicated class,extract translogdeletionpolicy to a dedicated class,enable a default translog retention policy, Extract Translog Deletion Policy for Simplicity and Future Expansion,extract translog deletion policy into a util class
" bugfix yes if relevant, link to documentation update: summary fixes #3484 no other information "," fix(ruleset): allow array of functions returning either string or loader objects  beautify, remove comments, debuggers ", crash with rule.use as a function returning array ,normalize mixed use array and function,allow array of functions returning either string or loader objects,allow array of functions returning string or loader objects,fix crash with rule.use as a function returning array, fix rule.use handling to support array of functions returning string or loader objects,fix crash with rule.use as a function returning array
" when running a custom install of chrome, and not downloading chromium (with the puppeteer_skip_chromium_download, image snapshots are unable to start up.  i ran across this issue when running storyshots inside a docker container already containing a chrome installation. add a config parameter to use a chrome executable path, instead of downloading chromium inside of puppeteer.  if no parameter is selected, defaults to undefined and operates as normal. add a parameter and check if it references the correct chrome.  in my case: /usr/local/bin/chrome ", feature: add config to supply executable chrome path to puppeteer  update readme ,,feature/config custom chrome executable path,add config parameter to use executable chrome path to puppeteer,fix image snapshots not starting when installing chrome,specify custom chrome executable path for puppeteer, add config for custom chrome executable path in puppeteer,feature: add config to supply executable chrome path to puppeteer
 cherry-pick #20781 #20780 #20912 #20824 ," [cherry-pick]fix bug in reshape: (#20781)  consider the situation that shape of input can contain more than one -1.  [cherry-pick]support tensor for split and concat, support -1 in num_or_sections, add check num_or_sections (#20780)  * improve split and concat op:  1. support tensor for argument 'dim' in split op.  2. support tensor for argument 'axis' in concat op.  * redefine function getdatafromtensor and set unknown output shape to - 1.  * add check: attr(sections) match input(x).  * support tensor for attr(sections) and attr(sections) can contain -1.  * modify error message and fix bug for concat and call resize only when necessary.  test=release/1.6 ",,"reshape,concat, split and squeeze",fix bug in reshape,"support tensor for split and concat, add check num_or_sections",[cherry-pick]fix reshape and split/concat ops, [cherry-pick]improve split and concat op support for tensors and -1 in num_or_sections,[cherry-pick]enhance split and concat op
" in underscore 1.1.4, a test case below will fail: var o = function(str) { return str; }; var fasto = _.memoize(o); equals(o('tostring'), 'tostring', 'checks hasownproperty'); equals(fasto('tostring'), 'tostring', 'checks hasownproperty'); i have fix the bug by changing code ""key in memo"" to ""hasownproperty.call(memo, key)"". hope to merge^o^ ", remove unused code and avoid variable redeclaration  bug fix for _.memoize when key is derived from prototype ,,bug fix for _.memoize and other little code change,fix for _.memoize when key is derived from prototype,bug fix for _.memoize when key is derived from prototype,fix bug in _.memoize when key is derived from prototype, fix _.memoize bug with prototype property checks,fix for _.memoize when key is derived from prototype
" fix: some refactor about datazoom and axis scale extent calculation: the ""scale extent calculation"" is depended by both coord sys and ""datazoom"". so it need to be performed in different stage of workflow and should avoid to call it repleatly or implement it repeatly. but previously, the ""scale extent calculation"" is implemented both in axishelper.ts and datazoom/axisproxy.ts, where different code implements similar logic. ""scale extent calculation"" is based on input of: (i) option specified min/max/scale (including callback) and (ii) dataextent and (iii) some filter related components like datazoom. currently we need add more filters depends on that axis scale calculation. e.g., (i) one axis min max effect the other axis extent (ii) custom filter. so we refactor that ""scale extent calculation"" as a reusable module (see scalerawextentinfo.ts). based on (1), make the callback of axis.min/axis.max consistent whatever it is called in ""data processing stage"" or ""coord sys updating stage"" based on (1), fix some ""inconsistent"" flaw in datazoom when handling stacked series. fix the type of axis.min/max to scaledatavalue. and enable the callback of axis.min/max to return scaledatavalue. remove some code that never used. make the ""data filter"", which will ""block stream"", not as the default data processor in the register api. feature: enable category axis min/max to shrink the other axis extent in cartesian. after this modification, if some data if out of the range of a category axis, the data item will not be filtered, but the extent of the other axis will be calculated based on the filtered data. if datazoom is used in either of the xaxis or yaxis in that cartesian, the shrink will not be performed. "," fix: some refactor about datazoom and axis scale extent calculation:  (1) the ""scale extent calculation"" is depended by both coord sys and ""datazoom"".  so it need to be performed in different stage of workflow and should avoid to call it repleatly or implement it repeatly.  but previously, the ""scale extent calculation"" is implemented both in axishelper.ts and datazoom/axisproxy.ts,  where different code implements similar logic.  ""scale extent calculation"" is based on input of:  (i) option specified min/max/scale (including callback) and  (ii) dataextent and  (iii) some filter related components like datazoom.  currently we need add more filters depends on that axis scale calculation.  e.g.,  (i) one axis min max effect the other axis extent  (ii) custom filter.  so we refactor that ""scale extent calculation"" as a reusable module (see scalerawextentinfo.ts).  (2) based on (1), make the callback of axis.min/axis.max consistent whatever it is called in  ""data processing stage"" or ""coord sys updating stage""  (3) based on (1), fix some ""inconsistent"" flaw in datazoom when handling stacked series.  (4) fix the type of axis.min/max to scaledatavalue. and enable the callback of axis.min/max to return scaledatavalue.  (5) remove some code that never used.  (6) make the ""data filter"", which will ""block stream"", not as the default data processor in the register api.  feature: enable category axis min/max to shrink the other axis extent in cartesian.  after this modification, if some data if out of the range of a category axis,  the data item will not be filtered, but the extent of the other axis will be calculated  based on the filtered data.  if datazoom is used in either of the xaxis or yaxis in that cartesian, the shrink will not be performed. ",,extent filtered by other axis,refactor about datazoom and axis scale extent calculation,some refactor about datazoom and axis scale extent calculation,fix/refactor: improve datazoom and axis scale extent calculation, Refactor DataZoom and Axis Scale Extent Calculation for Consistency and Efficiency,fix: some refactor about datazoom and axis scale extent calculation
" working on fixing:  my code follows the code style of this project: c, python i have read the pr checklist document and have made the appropriate changes. i have read the contributing document. "," convert tabs to spaces  use grid alignment for keycodes and macro arguments  rework layout macro  update layout macro to resemble the assembled layout.  use human-friendly formatting in info.json  update readme  update maintainer's account name, and add bootloader and flashing instructions. ",,misonoworks karina layout macro rework,update layout macro to resemble the assembled layout,update layout macro and add bootloader and flashing instructions,layout macro rework and updates, Layout Macro and Keycode Refinement with JSON Data Fix,ergocheap handwired layout macro refactor
 use updated fs api fix bug where file.m_file didn't exist in del() if we used file.close() first fs.removetree now removes tree as well as all tree contents (files). this is for a functionality sync between phantomjs and pyphantomjs all tests will now pass successfully , use updated fs api  fix bug where file.m_file didn't exist in __del__() if we used file.close() first  fs.removetree now removes tree as well as all tree contents (files). this is for a functionality sync between phantomjs and pyphantomjs  all tests will now pass successfully  regenerate resources ,,update and sync fs api,use updated fs api fix bug where file.m_file didn't exist in del(),use updated fs api and regenerate resources,remove fs api updates and bug fixes for phantomjs and pyphantomjs, update fs api and fix file deletion bug for phantomjs and pyphantomjs,update fs api and fix pyphantomjs file removal
 bpo-30320: test_eintr now uses pthread_sigmask() (#1523) test_eintr: fix resourcewarning warnings test_eintr: remove unused import bpo-25277: add a watchdog to test_eintr , bpo-30320: test_eintr now uses pthread_sigmask() (#1523)  rewrite sigwaitinfo() and sigtimedwait() unit tests for eintr using  pthread_sigmask() to fix a race condition between the child and the  parent process.  remove the pipe which was used as a weak workaround against the race  condition.  sigtimedwait() is now tested with a child process sending a signal  instead of testing the timeout feature which is more unstable  (especially regarding to clock resolution depending on the platform).  (cherry picked from commit 211a392cc15f9a7b1b8ce65d8f6c9f8237d1b77f)  test_eintr: fix resourcewarning warnings  (cherry picked from commit c50cccfcc3b3a9ef3fe7a78b7e7271930dc24aee)  test_eintr: remove unused import  bpo-25277: add a watchdog to test_eintr  set a timeout of 10 minutes in test_eintr using faulthandler. ,,backport test_eintr enhancements from master to 3.5,fix resourcewarning warnings in test_eintr,rewrite sigwaitinfo() and sigtimedwait() unit tests,backport test_eintr fixes from master to 3.6, Backport fixes for zombie processes and child reaping from master to 3.6,backport test_eintr updates from master to 3.6
 add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). provide a url to documentation or source code which provides context for the suggested changes: , upgrade to viewer v7.2  release notes:  upgrade to viewer v7.4  release notes:  added getdefaultgeometry method  added getdefaultgeometry method  added missing methods to navigation class:  - getworldpoint  - screentoviewport  added missing method to viewer3dimpl class:  - viewporttoray  added missing methods to navigation and viewer3dimpl classes  getdocumentnode returns any rather than object  added interface to specify api endpoint  move properties to top of the class  update type definitions for viewer. ,,add missing members and properties,add missing methods to navigation class,upgrade to viewer v7.2 and v7-3.0,[@types/forge-viewer] upgrade to viewer v7.4, Update type definitions for viewer and add missing methods to navigation and viewer3dimpl classes,[@types/forge-viewer]upgrade to viewer v7.4
" closes #24014 xref #10633 tests added / passed passes git diff upstream/master -u -- ""*.py"" | flake8 --diff whatsnew entry before this change, when the series.interpolate() method was called with invalid arguments,  exceptions with informative messages would be raised internally, but then caught and suppressed.  an exception with a potentially misleading message would be raised instead. for example, when interpolate is called with method='spline', an order argument must also be supplied.  this is checked internally, but when the order argument was missing, a confusing error would be raised: >>> import pandas as pd >>> s = pd.series([0, 1, pd.np.nan, 3, 4]) >>> s.interpolate(method='spline') traceback (most recent call last): ... valueerror: invalid method 'spline' to interpolate. this problem was reported as issues #10633 and #24014. after this change, a specific and correct exception (previously caught and discarded internally) is raised: >>> import pandas as pd >>> s = pd.series([0, 1, pd.np.nan, 3, 4]) >>> s.interpolate(method='spline') ... valueerror: you must specify the order of the spline or polynomial. in addition, light validation is now performed on the order parameter before it is passed to a scipy class.  previously, pandas would check if order was truthy in the python sense.  if order was non-zero and invalid, a scipy error would propagate to the user: >>> s.interpolate(method='spline', order=-1) traceback (most recent call last): ... dfitpack.error: (1<=k && k<=5) failed for 3rd argument k: fpcurf0:k=-1 after this change, a more understandable exception is raised in this case: >>> s.interpolate(method='spline', order=-1) traceback (most recent call last): ... valueerror: order needs to be specified and greater than 0 ", bug: raise accurate exception from series.interpolate (#24014)  actually validate order before use in spline ," unnecessary bare except at class block, function interpolate hides actual error ",fix exceptions when series.interpolate's order parameter is missing or invalid,raise correct exception from series.interpolate (#24014),raise accurate exception from series.interpolate (#24014),bug: series.interpolate() raises inaccurate exception when invalid arguments are provided, Improve error handling in series.interpolate for invalid arguments,series.interpolate raises more accurate exceptions for invalid arguments
" fixing the led code in my keymap to use both leds on the pro micro. before, it was incorrectly using pin b5 instead of d5. edited the led driving code to accurately use the leds on pins b0 and d5 instead of b0 and b5 as my previous code was doing. fixing use of incorrect pins for leds in my keymap. my code follows the code style of this project. i have read the contributing document. ", fixing led pins to accurately use the pro micro leds  fixing trailing whitespace ,,mf68 keymap led pins fixed,fixing led driving code in keymap,fixing my keymap to use pro micro leds,fixing led pins to accurately use the pro micro leds, Fixing LED Pin Usage in Pro Micro Keymap,fixing led pin usage in keymap
 don't checkout llvm-project don't require cmake and ninja fixes #78564 , don't checkout llvm-project when the llvm backend isn't built  don't require cmake and ninja when the llvm backend is not used , bootstrap: don't mandate cmake and ninja when the llvm backend is disabled ,misc rustbuild improvements when the llvm backend isn't used,don't checkout llvm-project when the llvm backend isn't built,don't require cmake and ninja when the llvm backend is disabled,update bootstrap to not require cmake and ninja when llvm backend is disabled, remove unnecessary requirements for cmake and ninja in llvm backend bootstrap,simplify the llvm bootstrap
" following the release of mistral-v1, we are pushing a draft pr with the stability fixes we made for training gpt-2 models directly to the base gpt-2 class definition (ensuring backwards compatibility). this is in line with the following issues: stanford-crfm/mistral#86: enabling sharing mistral checkpoints via hf hub (@osanseviero) #13463: upcasting scaled dot-product attention + layerwise scaling for stability (@lvwerra) concretely we implement: weight initialization from the original gpt-2 paper (by default, shouldn't affect folks unless they are training gpt-2 models from scratch) layer-wise scaling in scaled dot-product attention (optional flag; necessary for running/loading mistral gpt-2 models) scaled dot-product attention reordering (scale before dot-product) & fp32 upcasting when training in mixed precision (optional flag; only necessary for training new mistral/other gpt-2 models). this is a draft pr to aid in @lvwerra and @thomwolf's work training gpt-2 models stably; we plan on implementing tests (please let us know potential pain points), adding documentation, and will act on any other feedback you have . cc mistral team: @lorr1, @j38, @santhnm2 cc others at hf + gpt-2 model reviewers: @stas00, @patrickvonplaten, @lysandrejik resolves #13463 ", add layer-wise scaling  add reorder & upcasting argument  add layer scaling & upcast/reordering flags + functionality  add openai gpt-2 weight initialization scheme  openai gpt-2 initialization , upcasting of attention computation for reliable pretraining of gpt-2 models ,add mistral gpt-2 stability tweaks,gpt-2 weight initialization for reordering & upcasting,add support for training gpt-2 models stably,"stability fixes for gpt-2 training: weight initialization, layer-wise scaling, and upcasting"," Stability fixes for GPT-2 training - weight initialization, layer scaling, and attention upcasting","gpt-2: layer-wise scaling, upcasting, and reordering for mixed precision training"
" a proposed fix for #4802 i added 'replay' option to the playtoggle component. change has been verified in an actual browser (chome, firefox, ie) example created (starter template on jsbin) ", sync with origin branch  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  feat: add 'replay' option to the playtoggle component ,,add 'replay' option to the playtoggle component.,add'replay' option to playtoggle component,add'replay' option to playtoggle component,add'replay' option to the playtoggle component, add 'replay' option to the playtoggle component,add 'replay' option to the playtoggle component
 centralize all options at the root for easier discovery. renaming all_the_debug_macros  ->  enable_all_the_debug_macros for consistency with the rest of the options. making this enable_all_the_debug_macros an option makes it visible to users and tooling and formalize the type of the argument. making this build_lagoms an option makes it visible to users and tooling and formalize the type of the argument. , cmake: consolidate all options to the root of the project  cmake: remove some trailing whitespace from a few cmakelists.txt files ,,consolidate and document all options to the root of the project,cmake: consolidate all options to the root of the project,centralize all options to the root of the project,Consolidate all options to the root of the project for easier discovery, Consolidate and rename CMake options for better visibility and consistency,Consolidate all options to the root of the project
 this migrates all of our unit tests that were previously suffixed with .unit.test.js to a single test/unit folder removing the redundant suffix. a root tsconfig.json has also been added to enable type checking the newly converted tests to help prevent test errors from incorrect usage. , move unit tests to one folder  migrate unit tests to typescript ,,move unit tests to one folder and migrate them to typescript,migrate unit tests to one folder,migrate unit tests to typescript,Migrate unit tests to a single test/unit folder and enable type checking, migrate unit tests to typescript and consolidate test folder,move unit tests to test/unit and convert to typescript
" #39105 removed the normal unit testing from ci in favor of bazel, but the staging repos don't have build files in them because bazel choked on it.  this meant that unit tests weren't being run in ci (even though make test did it).  this restores the staging tests. @ixdy @spxtr since you were on the initial issue @liggitt @kubernetes/rh-cluster-infra thanks to @cheftako for finding it. ", fix up broken tests  restore unit testing for the staging repos ,,restore unit testing for staging repos,restore unit testing for staging repos,restore unit testing for the staging repos,add staging unit tests back to ci, restore unit testing for staging repos in ci,restore unit tests for the staging repos
" adds a bash script to automate changing into each of the examples/ directories and try to run npm i and gatsby build. this script is useful for trying to identify which of the example sites aren't working with the latest version of gatsby (using the next tag). some caveats: this script is slow. it currently takes hours to run, because it's building each example individually. some of the builds will fail because the folder structure for that example is different. e.g., examples/creating-source-plugins/ contains multiple gatsby sites, each in their own folder. this script won't catch all those edge cases. it's mainly an attempt to handle all the easy cases, while identifying individual failing examples that need to be investigated manually. examples/readme.md sc-38579 "," chore: delete outdated recipes examples  revert ""chore: delete outdated recipes examples""  this reverts commit 2dcb7858e149c754f9d31b7d9e8b03d7dd1fca31.  wip: attempt a build script in node (works for small subsets of examples)  chore: replace node build script with bash script (fixes parallelization crashing)  chore: improve documentation in build script ",,add a bash script to test building out examples,fix parallelization crashing in node,add bash script to automate changing into each of the examples,adds a bash script to automate changing into each of the examples/ directories and try to run npm i and gatsby build, chore: add bash script to automate building gatsby examples,
 follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). provide a url to documentation or source code which provides context for the suggested changes: since this pr #19777 got closed i picked it to provide the same fix increase the version number in the header if appropriate. i am not sure this is needed. i can fix it after code review note: i am quite new to typescript , fix onchange and onblur parameters  add semicolon  fix definition ,,redux form onchange onblur fix,fix onchange and onblur parameters,fix onchange and onblur parameters,fix definition and parameters for onchange and onblur, fix onchange and onblur parameters for pr #19777,[@types/react] fix onchange and onblur parameters
" this pr: lets you do python runner.py --help moves some potentially printing code into if __name__ == '__main__' prints help text as soon as possible, so some warnings will come after it lets you run tests from multiple suites on the same command line (e.g. python runner.py other.test_sixtyfour_bit_return_value asm2.test_time) and lists any missing tests - previously it just refused to run anything ensures that if you have 256 test failures it won't come back as 0! ", teach test runner about --help  try to execute less if importing the test runner  print help text first for a better chance of seeing warnings ,,allow running tests from different suites,teach test runner about --help,teach test runner how to print help text,rename cli flags to subcommands, Add --help option and improve test runner usability,"runner: improve --help, bug fixes"
 reduce many overheads in saving and loading multi-threading save and load still use text format. binary format will be added after this pr. both saving and loading are about 10x faster now. , remove protobuf  add version number  remove pmml script  use float for split gain  fix warnings  refine the read model logic of gbdt  fix compile error  improve decode speed  fix some bugs  fix double accuracy problem  fix bug  multi-thread save model  speed up save model to string  parallel save/load model  fix some warnings. ,,speed up saving and loading model,save and load multi-threading save and load,reduce overheads in saving and loading,fix multi-threading save and load performance, Speed up multi-threaded save/load model operations,speed up save/load model
" see #18751 for more info on the bug. this changes the ref counting protocol to make sure that workers report any nested objectrefs that are still in scope. the main issue is that we need to make sure to eventually clean up all objectrefs that have gone out of python scope, but we also need to make sure to account for all nested objectrefs that might still be in scope. this pr uses a per-object flag to indicate whether it has any nested objectrefs that might still be in scope. the flag gets updated recursively whenever a nested objectref is used. when an object ref goes out of scope and the borrower responds to the waitforrefremoved rpc, the reply includes all nested refs and we reset the flag. then, we just have to make sure not to delete an objectref if its flag is set. this should not result in any additional messages, but individual messages may be larger because they can include object refs that have already gone out of scope and don't need to be reported. i chose not to optimize this for now. closes #18751. i've run scripts/format.sh to lint the changes in this pr. "," fix assertion crash  test, lint  todo  tests  protocol  test  fix  lint  header  recursive  note  forward test ", [bug] ref count safety bug when inlining objectrefs or deserializing objectrefs multiple times ,fix bug in ref counting protocol for nested objects,ref count safety bug when inlining objectrefs or deserializing objectrefs multiple times,ref count safety bug when inlining objectrefs or deserializing objectref objects multiple times,fix ref counting protocol to account for nested objectrefs, fix ref counting safety bug for nested objectrefs,fix ref counting assertion check when handling nested objectrefs
" adding types declaration for module slashy   add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). the package does not already provide its own types, or cannot have its .d.ts files generated via --declaration create it with dts-gen --dt, not by basing it on an existing project. represents shape of module/library correctly tslint.json should be present and it shouldn't have any additional or disabling of rules. just content as { ""extends"": ""dtslint/dt.json"" }. if for reason the some rule need to be disabled, disable it for that line using // tslint:disable-next-line [rulename]  and not for whole package so that the need for disabling can be reviewed. tsconfig.json should have noimplicitany, noimplicitthis, strictnullchecks, and strictfunctiontypes set to true. ", add splashy types definitions  add splashy test ,,add types declaration for module splashy,add types declaration for module slashy,add types declaration for slashy,add types declaration for module slashy, add types for splashy module,add types for module slashy
" abstract introduces a new option --fragment-retries for retrying fragments in fragment based downloaders (yet dash segments only) with default value of 10 retry attemps. rationale youtube may often return 404 http error for a fragment in dash segments downloader causing the whole download to fail. however if the same fragment is immediately retried with the same request data this usually succeeds (1-2 attemps is usually enough for success) thus allowing to download the whole file successfully. so, we will retry all fragments that fail with 404 http error for now. example urls example videos i've been able to reproduce failing fragments with: rtwddp2uiu0 imiqwgpbquq. videos uploaded to youtube in a last hour are also very likely to be dash segments with failing fragments. demo here is how it looks in action: > py26yt -v  deleting original file test online game. bigdaimon vs _shepherd-imiqwgpbquq.f160.mp4 (pass -k to keep) deleting original file test online game. bigdaimon vs _shepherd-imiqwgpbquq.f140.m4a (pass -k to keep) further improvements this can be adopted for other fragment based downloaders if necessary. "," [options] add --fragment-retries option  [downloader/fragment] add report_retry_fragment  [downloader/dash] add fragment retry capability  youtube may often return 404 http error for a fragment causing the  whole download to fail. however if the same fragment is immediately  retried with the same request data this usually succeeds (1-2 attemps  is usually enough) thus allowing to download the whole file successfully.  so, we will retry all fragments that fail with 404 http error for now.  [downloader/{common,fragment}] fix total retries reporting on python 2.6  [downloader/fragment] document fragment_retries ",,add --fragment-retries option (fixes #8466),add fragment retries option for fragment based downloaders,add --fragment-retries option for fragment based downloaders,add --fragment-retries option for retrying fragments in fragment based downloaders, add --fragment-retries option for retrying fragments in dash segments,add --fragment-retries option for dash downloader
" test sharding appears to have been broken by commit 87cc788. the result being that many tests are simply not run when sharding is enabled. the issue seems to be that the filtering for sharding is happening twice, once in tensorflow/python/platform/googletest.py and then once again in external/absl_py/absl/testing/absltest.py. this commit fixes the issue by removing the shard filtering code in googletest.py. fixes: #25594 "," fix test sharding  test sharding appears to have been broken by commit 87cc788.  the result  being that many tests are simply not run when sharding is enabled.  the issue seems to be that the filtering for sharding is happening  twice, once in tensorflow/python/platform/googletest.py and then once  again in external/absl_py/absl/testing/absltest.py.  this commit fixes  the issue by removing the shard filtering code in googletest.py.  fixes:  remove unused import of itertools ", test sharding appears to be broken ,fix test sharding version 2,"fix test sharding, remove unused import of itertools",fix test sharding and remove unused import of itertools,fix test sharding and remove unused import, fix test sharding and remove unused import of itertools,fix test sharding
" changes in this: if the old state was not present or not used, we set flag that says instead of calculating signature for a file (which is dts emit hash) use the version of file as the signature. this makes it so that initial compilation will not calculate the d.ts emit and next file change to the file will be treated as non local change and thats when the signatures for file reference dependency are calculated and it would probably result in emitting more files than necessary but thats the compromise for not having to spend cost for d.ts emit in the initial round. the next change to that file should be able to correctly detect local/non local change as before. 925e70e just updates tests to ensure we are testing the scenarios we intended. dd1cef2 is actual change and may be ideal to look at the changes as part of that commit. 69ebfe3 checks updates to the incremental correctness of the program, essentially signature is same as d.ts emit signature or version of the file. also exported modules map is checked in similar way. 44ba0ec reverts the compileonsave to old behavior of always computing signature this is simplified implementation of work in #42960 by @sokra there are more todos that we can improve on, eg if global file is changed, mark it for lazy signatures etc  but i think that each change should be separate change to be able to evaluate the perf impact and if needed revert it. potential improvements: global file change => resulting in emitting all files so could benefit from using signature as version certain number of new files percentage certain number of changed file percentage ", extra tests in preparation for lazy signature making sure the original intent of test is maintained  whenver we cant use state delay signature calculation and use source file version as signature  incremental correctness checks ,,do not calculate signatures if old state is not used,revert compileonsave to old behavior of always computing signature,always use the version of file as signature,fixes the issue with emit where in same file is emitted multiple times, Delay signature calculation for initial compilation and use source file version as signature,simplify signature calculation for d.ts emit
" adds support for via configurator to the southpaw fullsize by switchplate peripherals. adds support for via configurator to the southpaw fullsize my code follows the code style of this project: c, python i have read the pr checklist document and have made the appropriate changes. i have read the contributing document. "," begin work on spfs, migrating ancient config  qmk breaks if there's a dash in the board name  update info.json  make indicator leds work  build a readme  change title to match official gb thread name  add an ansi wkl layout for simplicity  adjustments as per pullreq recommendations  remove unused functions from other keymap  add via configurator support  remove layers to fix via  re-add extra via layers since it seems to work now  replace tabs with spaces  update readme.md ",,add via configurator support to southpaw fullsize,add via configurator support to the southpaw fullsize,add via configurator support to southpaw fullsize,adds support for via configurator to the southpaw fullsize, add via configurator support to southpaw fullsize by switchplate,add via configurator support to southpaw fullsize
" get things in a state where the stdlib build passes as well as tests, but do not actually enable constraint propagation yet. "," [constraint solver] disabling the shrink() pass results in new ambiguities.  one expression in this test becomes ambiguous when the shrink() pass is  disabled. enabling the constraint propagation pass disables the shrink  pass.  for now we'll run this with -propagate-constraints explicitly enabled  and with the expected output changed. this is a regression that will  need to be investigated and fixed in the solver.  [constraint solver] tweak test based on using a bit more memory for -propagate-constraints.  this test is trying to confirm that memory usage is independent and that  follow-on expressions do not fail just because of the first failure, and  now we naturally fail on another expression because of the additional  memory used for -propagate-constraints.  tweak the test to bump up the threshold and swap the order of  expressions so that the now-failing expression is first.  [constraint solver] fix memory corruption issue.  we use simplifyconstraint() to activate other constraints, and then  examine those constraints to find related disjunctions. in examining  those active constraints, we were simplifying them in case they  failed (which would allow us to bail out earlier). in doing so, we could  potentially generate new disjunctions when we simplify an unresolved  value member constraint. if we do that, we end up collecting these new  disjunctions as part of the set of related disjunctions, but that's  problematic because as part of exiting the solver scope to roll back  changes we delete these disjunctions from the system.  instead of actually simplifying the active constraints, just collect the  disjunctions and move the active constraints back to the inactive list.  with this change we can build the stdlib. ",,fixup some issues with constraint propagation,fix memory corruption issue in solver,enable -propagate-constraints in the stdlib build,"Get the constraint solver in a state where the stdlib build passes and tests pass, but do not enable constraint propagation yet.", Prepare for constraint propagation without enabling it yet,constraint solver updates for stdlib build
" this pr updates the bottom navigation semantics tests to use the matchessemantics api, which is the newer function for accomplishing the same as the previous tests. i added the following tests: 4 variants of semantics tests in the bottom navigation bar tests. before you create this pr confirm that it meets all requirements listed below by checking the relevant checkboxes ([x]). this will ensure a smooth and quick review process. i read the contributor guide and followed the process outlined there for submitting prs. i signed the cla. i read and followed the flutter style guide, including features we expect every widget to implement. i read the tree hygiene wiki page, which explains my responsibilities. i updated/added relevant documentation (doc comments with ///). the analyzer (flutter analyze --flutter-repo) does not report any problems on my pr. i am willing to follow-up on review comments in a timely manner. did any tests fail when you ran them? please read handling breaking changes. no, no existing tests failed, so this is not a breaking change. yes, this is a breaking change. if not, delete the remainder of this section. i wrote a design doc:  i got input from the developer relations team, specifically from: replace with the names of who gave advice i wrote a migration guide: ", update semantics tests ,,update bottom nav semantics tests to use matches semantics,update bottom navigation semantics tests,update bottom navigation semantics tests,update semantics tests for bottom navigation bar, Update Bottom Navigation Semantics Tests with MatchesSemantics API,update bottom navigation semantics tests
" hope this is fine. i replaced the star import in tests.serializers as ""explicit is better than..."" (and code validators like pyflakes don't work very well with them). i also added a test that makes sure that all fields from the fields meta-attribute are passed as expected. ", bye bye star import  added test that makes sure that fields with  dictionaries as data are returned as expected and  not turned into string representations  added test for modelserializer meta fields  returning as expected ,,"tests for pull #358 ""return dictionaries as is""",bye bye star import added test for meta fields returning as expected,replaced star import in tests.serializers and added test,add support for model serializer meta fields, replace star imports with explicit imports in tests and add missing field validation tests,fix star import in tests.serializers
 small fixes in the readme conform to the current version of the boilerplate. updated jquery to version 3.1.1 and updated some dependencies to their latest versions. , fixed small grammar error  replaced .jade with .pug  small changes conform to the current version of the app  small changes conform to the current version of the app  updated jquery to version 3.1.1  updated jquery to version 3.1.1  updated some dependencies ,,update readme conform to current version & other small updates,small fixes in the readme,small fixes in the jquery readme,fixes in the readme conform to the current version of the boilerplate, updated jquery and dependencies in boilerplate,small updates
" there are some random failures of apiv3 tests during ci on github (github actions). this is probably caused by clashes of computed identifier of test documents. these clashes probably arise in very fast (or parallel) test execution, when test documents get the same timestamp. this pr tries to solve this by isolating test documents, which is achieved by using different device name in different test file. ", apiv3: isolating documents from tests (not allowing clashes of calculated identifiers)  removing unused async keyword ,,trying to fix random fail of apiv3 tests,apiv3 tests are not allowing clashes of computed identifiers,separating documents from tests (not allowing clashes of calculated identifiers),fix apiv3 test failures due to computed identifier clashes, fix random failures in apiv3 tests by isolating test documents,fix apiv3 tests clashes of computed identifier
 leaving the pad or the invalid_entry aba_ctr uninitialized causes valgrind to complain since part of the atm read is still uninitialized. possibly consider doing this under ndebug? , stop upsetting valgrind with uninitialized shorts  add explicit comments about purpose of writing to pad ,,write to dummy pads and invalid entries in lock-free stack to prevent valgrind complaints,stop upsetting valgrind with uninitialized shorts,stop upsetting valgrind with uninitialized shorts,eliminate some uses of uninitialized memory found by valgrind, stop upsetting valgrind with uninitialized shorts,stop upsetting valgrind with uninitialized shorts
 this pr converts many but not all of the uses in the codebase. as a result building rust issues a lot of warnings. i didn't have time to fix all those...i'll try to fix more but i thought it made sense to post the pr anyway. , declare &foo[] to be obsolete syntax. modify the obsolete mechanism to  support warnings.  replace all uses of &foo[] with &foo[..] en masse. ,,make &foo[] syntax issue a warning (but still work),replace all uses of &foo[] with &foo[..],replace all uses of &foo[] with &foo[..] en masse,fix rust warnings by converting &foo[] to &foo[..], convert &foo[] to &foo[..] to reduce build warnings,declare &foo[] to be obsolete syntax
 games that use the depth test will most likely draw a black screen until depth clearing is implemented. see #609 , gpu: implemented the z24s8 depth format and load the depth framebuffer.  gpu: added registers for depth test and cull mode.  maxwelltogl: added conversion functions for depth test and cull mode.  gpu: set up the depth test state on every draw. ,,implemented the depth buffer and depth test + culling,gpu: added depth test and cull mode,implemented the z24s8 depth format and load the depth framebuffer.,fix depth test implementation, Implement Depth Test and Clear for Game Rendering,implement depth test
" this pr fixes #13767 adding in additional interfaces to correctly define certain easing factories, which allow parameterization (i.e. poly, back and elastic) it is related to #11365 and #11366. thanks for spotting the bug @billdwhite make your pr against the master branch. follow the advice from the readme. avoid common mistakes. run tsc without errors. run npm run lint package-name if a tslint.json is present. provide a url to documentation or source code which provides context for the suggested changes: < increase the version number in the header if appropriate. if you are making substantial changes, consider adding a tslint.json containing { ""extends"": ""../tslint.json"" }. "," fetch-merge commit from dt master  merge remote-tracking branch 'definitelytyped/master'  merge remote-tracking branch 'definitelytyped/master'  merge remote-tracking branch 'definitelytyped/master'  merge remote-tracking branch 'definitelytyped/master'  merge remote-tracking branch 'definitelytyped/master'  merge remote-tracking branch 'definitelytyped/master'  d3-ease:  * [fix]: added interfaces and changed definitions for three classes of easing functions, which can be parameterized. fixes #13767.  * [enhancement]: activated strictnullchecks after validation  * [chore]: added jsdoc comments  * [chore]: added linting config file. ", [bug] d3-ease omitted easing function parameterizations ,"add parameterization, strictnullchecks and  jsdoc comments",add interfaces and changed definitions for easing functions,d3-ease added missing easing function parameterizations,add support for easing factories in d3-ease, fix d3-ease easing function parameterizations and add strict null checks,"d3-ease: added interfaces and changed definitions for three classes of easing functions, which can be parameterized. fixes #13767."
" runtime tested on rpi4 with justboom dac, justboom digi and hifiberry digi-pro that's it from my side, i think i'm through all the i2s cards i have here ", asoc: justboom-dac: use modern dai_link style  asoc: rpi-wm8804-soundcard: use modern dai_link style ,,"justboom-dac, rpi-wm8804-soundcard: use modern dai_link style",use modern dai_link style asoc,use modern dai_link style for rpi4,alsa/soc/bcm: update to use modern dai_link style for JustBoom DAC and RPi WM8804 soundcard, asoc: update dai_link style for justboom-dac and rpi-wm8804-soundcard,alsa/soc: add asoc support for justboom dac and rpi-wm8804-soundcard
" this pull request implements following changes add hindi translation for weather report, morning, noon, evening, night add hindi translation for help file and could you please add your name to the translators list? (see above) closes: #429 ", feat(translation): hindi translation for we-lang  feat(translation): add hindi translation for help file , hindi translation ,hindi translation for we-lang and help file,add hindi translation for we-lang,add hindi translation for weather report and help file,hindi translation for weather report and help file," added hindi translations for weather report, help file, and time periods","add hindi translation for weather report, help file and update translators list"
" remove a user from a team, or as a regular user, leave the team. the last modal step should be the one in the screenshot. ", update leave team modal  stories  remove user modal ,,change modals for remove user from team && leave team,remove user modal from team,update leave team modal,update leave team modal to new design, update leave team modal and remove user modal,update leave team modal
 implement various traits (iterbytes and extra's encodable and decodable) for rc when t alreay implements the trait. , add an implementation of encodable and decodable for rc. this will be needed to use rc in place of @ in libsyntax.  implement iterbytes for rc<t>. ,,implement various traits for rc<t>,implement iterbytes and extra's encodable and decodable for rc,implement iterbytes and decodable for rc<t>,Implement various traits for rc," Implement encodable, decodable, and iterbytes traits for rc",make rc implement required traits
" part 8. we have an in-house rule to compare explicitly against false instead of using the logical not operator (!). however, this hasn't historically been enforced, meaning that there are many violations in the source at present. we now have a checkstyle rule that can detect these cases, but before we can turn it on, we need to fix the existing violations. this is being done over a series of prs, since there are a lot to fix. ", more fixes in libs/  fixes in x-pack/plugin/sql  fixes in server action/admin/indices  fixes in client and server  fixes in server  fixes in server  fixes in server  fixes in server  fixes in server  fixes in server  more fixes ,,replace not operator with explicit false check - part 8,more fixes in libs/ fixes in x-pack/plugin/sql,fix a bunch of violations in the source,replace not operator with explicit false check - part 8, replace not operator with explicit false check - part 8,replace not operator with explicit false check - part 8
" fixes #7189  (regression introduced with #7102) additionally ensured old function arn format, when no provisioned concurrency setup is involved, this should help partially solve issues for users of plugins which relied on old template format -> davidgf/serverless-plugin-canary-deployments#71 ", refactor(aws lambda): ensure natural function reference when no alias  test(aws lambda): improve coverage  expose #7189  refactor(aws lambda): resolve deep value once  refactor(aws lambda): do not deep merge when not necessary , breaking change introduced for custom authorizer lambdas ,fix lamdba permissions setup when authorizer is involved,fix refactor(aws lambda): do not deep merge when not necessary,ensure natural function reference when no alias,fix natural function reference when no alias, fix custom authorizer lambda reference format,ensure proper function ref when no alias
" this pr ensures that handlers have the correct parent.  this change does for handlers what tasks already had done. this ensures that things like the following apply to handlers: - import_role: name: thing delegate_to: localhost fixes #36518 additionally, on a small note, task_list.extend is in place and returns nothing, don't assign to t. ansible version ", ensure role handlers are parented correctly. fixes #36518  add delegate_to test for include_role handlers , import_role with delegate_to does not run handlers on the delegated to host ,ensure handlers have proper parent,ensure role handlers are parented correctly,ensure role handlers are parented correctly,clean up role handlers to ensure correct parenting, Ensure correct parentage for role handlers and fix delegate_to behavior,ensure role handlers are parented correctly
 @mugen87 there were only minor corrections to the ts introduced with #16969 i realized that codeserializer was poorly documented and that it needed clean-up especially with regard to unused functionality. that's why it became the biggest change here. ," codeserializer: clean, update doc and typescript definitions  general: align js docs and ts signature files  codeserializer: clean-up/remove functionality no longer needed. make override clearer via codeserializationinstruction  fixed comments and made minor ts adjustments ",,"clean-up, code doc update and ts alignment",fix minor ts corrections,minor clean-up and ts fixes,fix codeserializer documentation and typescript definitions, improved documentation and typescript definitions for codeserializer,"codeserializer: clean-up, doc updates and ts definitions"
" with this pr, when a node is the left hand expression of a property access, element access, or call expression, and the type of the node includes type variables with constraints that are nullable, we fetch the apparent type of the node before performing control flow analysis such that narrowings apply to the constraint type. for example: type item = { (): string; x: string; } function f1<t extends item | undefined>(obj: t) { if (obj) { obj.x;     // ok obj[""x""];  // ok obj();     // ok } } prior to this pr, the property access, element access, and call expressions above were errors. fixes #14091. fixes #14415. ", obtain apparent type before narrowing type variables  new behavior only for type variables with nullable constraints  add tests ,,improve type guards for type variables,obtain apparent type before narrowing type variables,get apparent type before narrowing type variables,improve control flow analysis for nullable type variables, Improve control flow analysis for nullable constrained type variables,"fetch apparent type before narrowing for property accesses, element accesses, and calls"
 moves the python tests to the python subdir. adds a libbcc.a target for those statically inclined. ," move python tests to tests/python  they didn't quite make sense where they were. at one point 'cc' meant  compiler, and yes they were testing the compiler. now lets use the  convention that different tests/ subdirectories test different bindings  (c, python, other).  add libbcc.a compilation and test binary  this creates a new static library target for bcc. add one trivial c test  case that links against it. ",,move cc/*py tests to python subdir and introduce c standalone test binary,add libbcc.a compilation and test binary,move python tests to python subdir,move python tests to tests/python and add libbcc.a target, Move Python tests to subdir and add libbcc.a target,"add python tests, add libbcc.a static library target"
" mysqlnd_field_type_name is not used anywhere. my research turned up nothing. it looks like a leftover from some previous functionality. either i am too dumb to understand or get_parameter_metadata is just dead code. i can't see any references to this method anywhere. stat triggers is a functionality that doesn't seem to be used anywhere. it doesn't look like it harms performance, but if nobody uses it then it might be prudent to just drop this functionality and have cleaner code. "," remove mysqlnd_field_type_name  remove get_parameter_metadata  drop mysqlnd statistics triggers  this functionality is not used productively in php and it's not used in  any of the extensions to my knowledge. since it looks like this functionality  isn't required by anyone, let's clean up mysqlnd and drop it. ",,"mysqlnd refactoring (remove mysqlnd_field_type_name, get_parameter_metadata, and mysqlnd stat triggers)",remove mysqlnd_field_type_name and drop stat triggers,drop mysqlnd stats triggers and stat triggers,Clean up unused mysqlnd functionality, remove unused mysqlnd_field_type_name and get_parameter_metadata methods,Remove unused mysqlnd functionality
" this pr offers a fix for pr 16162's problem with causing td3 tests to fail/timeout often. pr 16162 added a sleep to _nextvaluenotready causes td3 tests to become flakey. this pr removes that sleep from the c'tor of _nextvaluenotready, but keeps the sleep in place inside impala's learner_thread such that the original issue should remain fixed. see also this discussions here:  i've run scripts/format.sh to lint the changes in this pr. ", wip.  wip. ,,having added sleep to _nextvaluenotready causes td3 tests to become flakey.,fix td3 tests to fail/timeout often,fix for pr 16162's problem with causing td3 tests to fail/timeout,fix td3 tests flakiness by removing sleep from _nextvaluenotready, Fix TD3 Test Flakiness by Adjusting Sleep in _nextValueNotReady,fix td3 flakiness introduced by #16162
" add or edit tests to reflect the change. (run with npm test.) run npm run lint package-name (or tsc if no tslint.json is present). the package does not already provide its own types, or cannot have its .d.ts files generated via --declaration create it with dts-gen --dt, not by basing it on an existing project. tslint.json should be present, and tsconfig.json should have noimplicitany, noimplicitthis, strictnullchecks, and strictfunctiontypes set to true. ", feat: react hooks helper types  fix: react hooks helper test ,,types for npm package: react-hooks-helper,react hooks helper types fix,add types for react hooks helper,add types for react hooks helper, feat: react hooks helper types and fix: react hooks helper test,Feat: Add types for React Hooks helpers
 please don't merge this pr until cocos2d/cocos2d-js-tests#201 is merged. ," issue #2521: updating comments where searchs full path.  issue #2521: capture event of controlbutton in ccb for jsb.  issue #2521: fixing a logical error in ccbreader::addowneroutletnode. 'null  != node' --> 'null == node'.  issue #2521: removing an unused method controlstepper::setvalue(double value, bool send); since it's not implemented and there is an similar method 'setvaluewithsendingevent' could replace it.  issue #2521: testjavascript needs ccbreader support.  issue #2521: adding 'cc.convertcolor3btohexstring' in jsb_cocos2d.js.  issue #2521: updating tojs/cocos2dx_extension.ini, bind more cccontrols.  issue #2521: reporting an error when the callback function is undefined in 'js_cocos2dx_setcallback'.  closed #2521: updating jsb_cocosbuilder.js, it supports cccontrol now.  issue #2521: updating js-test. ",,adding more extensiontest like cocosbuildertest and controlbuttontest and bug fix in ccbreader.,cccontrol support for jsb,"updating tojs/cocos2d-js-tests, bind more cccontrols.",upgrading jsb_cocosbuilder.js to support cccontrol., Updating JSB support and fixing logical errors in CCBReader,cocos2d-js-tests
 layername buttons will grow/shrink to fill layer panel width. grows to a maximum of the draw width of the widest set layer name. also fixes #47 ," fix stbte_create_map declaration  stbte: layername button grows/shrinks  layer name buttons grow to fill box  stbte: update documentation/version 0.31  changed revision history, todo, credits, and readme ", stbte_create or stbte_create_map ,layer name buttons grow with layer panel,add layername buttons to fill box,stbte create and create map,fixed layername buttons to grow/shrink to fill layer panel width, improved layer name button sizing and fixed #47,stbte_create: layername button grows/shrinks
" some package repositories which require basic http authentication include the user's credentials in urls returned as part of json responses. because poetry's request authentication system checks for a matching netloc, this ends up breaking basic http authentication for such repositories. this pull request compares by hostname instead of netloc. because hostname doesn't include any extra url authentication information, this fixes access to these repositories. this fixes issue #746. ", fix authentication failure for some apis  fixes an issue which occurs when apis return authentication information in the urls embedded in json responses.  test authentication on same host with duplicated credentials  add test to ensure that basic authentication credentials in request urls does not break authentication when they credentials match. this test purposefully does not examine program behavior when url credentials and session credentials are different. ,,fix request authentication when credentials are included in urls,fix authentication failure for some apis,fix authentication failure for some package repositories,make poetry's request authentication system work with basic http authentication in urls, fix basic http authentication for repositories with embedded credentials in URLs,fix basic http authentication with urls containing authentication information
" the adder node can now add float vectors of different lengths by trimming them to the same dimensionality. we can change this behavior in the future, or introduce different nodes (like a ""mix"" node) that address differing vector lengths in different ways. ", introduce dimensions to expression  rename to setexpressionforslot and add redcomponent function  add expression helpers to aid with differing data widths  add float2constantnode  single-line functions when possible ,,introduce handling for varying dimensionality,add float vectors of different lengths,add float vectors of different lengths to expression,Add support for adding float vectors of different lengths, Add support for adding float vectors of different lengths in Adder Node,allow adder node to add vectors of differing lengths
" @rocketchat/core closes #7408 this feature allows someone with ""view-room-administration"" permission to list all groups with the api call /api/v1/groups.list similar modifications should be done in the other groups api calls to ensure users with the proper permissions can do the same thing via api than with the web interface (such as delete & modify groups) ", fix #7408  indent fixes ,,allows admin to list all groups with api,fix #7408 indent fixes for v1/groups.list,allow view-room-administration permission to list all groups,add permission checks to groups api methods, Add permission checks to bulk REST API methods,add groups.list api call
" i tried to translate the original file into arabic, but there are some blocks that cannot be translated into arabic because the meaning is very different, for the tables, i could not translate them completely ", add the ds link  finish translating math  add arabic version of the readme file  add arabic version of the readme file  add the arabic readme file link to main readme file  add the arabic readme file link to main readme file  add the arabic readme file link to main readme file ,,add an arabic version of the readme file,add arabic version of the readme file,add the arabic version of the readme file,add arabic translation to readme, add arabic translation for readme with partial table translation,add arabic translation for readme file
 this should tidy up the javadoc generation and the hystrix-contrib project. ," use classpath, which is being changed by provided, instead of default  nebula-30 fixing javadoc arguments. using proper delegate in javadoc closure, add custom string option  nebula-28 preventing contrib project from being uploaded  nebula-31 ignore output from gradle maven ant tasks ",,"fixes for nebula-28, nebula-30, nebula-31",tidy up javadoc generation and hystrix-contrib project,fix javadoc generation and hystrix-contrib project,Tidy up javadoc generation and hystrix-contrib project, Tidy up javadoc generation and hystrix-contrib project,build changes to nebula.netflixoss
" this is a basic change.  we have nightwatchtesthooks extending nightwatchglobals, but the globals property is only scoped to nightwatchglobals.  this excludes global before, beforeeach, after, and aftereach functions. it's important to separate before/after and beforeeach/aftereach - they don't get the same arguments. add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). provide a url to documentation or source code which provides context for the suggested changes:  include tests for your changes if you are making substantial changes, consider adding a tslint.json containing { ""extends"": ""dtslint/dt.json"" }. if for reason the any rule need to be disabled, disable it for that line using // tslint:disable-next-line [rulename] and not for whole package so that the need for disabling can be reviewed. ", fixed page_object_path to page_objects_path  added webdriver options to nightwatchoptions  forgot a semicolon  merge remote-tracking branch 'upstream/master'  changed globals definition to nightwatchtesthooks  separated global hook definitions ,,changed globals from nighwatchglobals to nightwatchtesthooks,changed globals definition to nightwatchglobals,changed globals definition to nightwatchglobals,update nightwatchtesthooks to include global hook definitions, update globals and hooks in nightwatchtesthooks,separate global hook definitions
" rename container_option to container, make runtime options have the same code style. rename image name from ray-nest-container to ray-worker-container,  because it can be used in other scenarios, not only in nest container. #16671 i've run scripts/format.sh to lint the changes in this pr. ", rename container_option to container  rename ray-nest-container to ray-worker-container ,,rename container option and ray-nest-container,rename image name from ray-nest-container to ray-worker-container,"rename container_option to container, make runtime options have the same code style",rename container_option and image name, rename container_option to container and update image name,rename container related variables
" addresses the handful of remaining feedback from pt. 2, plus adds two new tests: one verifying a multi-topology application with a fkj and its internal topics, another to make sure iq works with named topologies (though note that there is a bit more work left for iq to be fully supported, will be tackled after pt. 3 "," followup to feedback from pt. 2, cleanup some tests and peel some pt. 3 refactoring up front, plus test for iq  clarify iq status ",,minor followup from pt. 2 and some new tests,pt. 2 followup to pt. 2 feedback,followup to feedback from pt. 2,fix remaining feedback from pt. 2 and add tests for multi-topology application and named topologies, add tests for multi-topology application with FJK and named topologies,"followup to feedback from pt. 2, cleanup some tests and peel some pt. 3 refactoring up front, plus test for iq"
" this issue if related to #5546, where it is claimed that datafeeders will become deprecated. as such, this branch has following changes added _generatorfeedfn class generator_input_fn in generator_io.py support for generator_input_fn in enqueue_data() added unitest in generator_io_test.py refactored code keeping indent spacing to 2. ", branch to test_tensorflow  using types.generatortype for validation  changed to receving generator function  this allows to run multiple epochs of a finite generator  using placeholders as keys to feed dictionary  added correct counter for the index_placeholder  added corrections to tests  removing extra spaces ,,add support for dict generator input_fn in learn_io,added _generatorfeedfn class generator_input_fn in generator_io.py,support for datafeeders in generator_io.py,configurable generator input function for datafeeders, added support for generator_input_fn in TensorFlow data feeders,Test Tensorflow with Generator Functions
" commit message: in order to speed up eds, don't necessarily visit every proto field to count its validity as warningvalidationvisitor does. this yields a ~30% speed improvement in processing very large updates in eds. risk level: medium, new feature behind a command line flag. testing: unit and bechmark tests. docs changes: these are probably wrong, thus the draft-ness. release notes: eds can now ignore unknown dynamic fields, for a ~30% improvement in update processing time. behind --ignore-unknown-dynamic-fields ", add the ability to ignore unknown fields in eds.  repair some merge damage and move the conditional down a layer as htuch requested. ,,optionalize counting of unknown fields,eds can now ignore unknown dynamic fields,ignore unknown dynamic fields in eds,add ability to ignore unknown dynamic fields in eds for a ~30% speed improvement, Speed up EDS processing by ignoring unknown dynamic fields with --ignore-unknown-dynamic-fields flag,eds: speed up validation of service updates by ignoring unknown dynamic fields
" after the merging of #9848 , we can now interact with the jsonapi endpoints of a drupal site behind basic authentication. this works, however gatsby cannot retrieve remote files, because the calls executed in createremotefilenode do not currently take into consideration the basic auth credentials. this pull request adds those credentials so that files are correctly fetched from the remote drupal site behind basic auth. ", gatsby-source-drupal: use basic auth credentials also when fetching remote files.  gatsby-source-drupal: better support the case where no basic auth is specified. ,,use basic auth credentials to fetch remote files as well.,use basic auth credentials when fetching remote files,use basic auth credentials also when fetching remote files,add basic auth credentials for remote file fetching, Improve Gatsby-source-Drupal to handle basic auth for remote file fetching,gatsby-source-drupal: add support for basic authentication when fetching remote files
" closes #9906 done the cross linking as mentioned in the issue linked above. i have reviewed my changes in staging (look for the latest deployment event in your pull request's timeline, then click view deployment). for content changes, i have completed the self-review checklist. ", add further reading links  fixes github#9906  fix link  add further reading links  fixes github#9906  linking  fixes github#9906  linking  linking  added further reading section ," add ""further reading links"" between end-user and org guides for codespaces ",adds further reading links to certain codespaces docs,add more reading links to end-user and org guides,"add ""further reading links"" for codespaces",adds further reading links between end-user and org guides for codespaces," add ""further reading links"" between end-user and org guides for codespaces",add further reading links between end-user and org guides for codespaces
" remove duplicate code in abstractconfig.java to make brief and clean remove code classutils.isprimitive(method1.getreturntype()) in abstractconfig.java at line 653 classutils.isprimitive(method1.getreturntype()) is already judge in method methodutils.isgetter follow this checklist to help us incorporate your contribution quickly and easily: make sure there is a github_issue field for the change (usually before you start working on it). trivial changes like typos do not require a github issue. your pull request should address just this issue, without pulling in other changes - one pr resolves one issue. format the pull request title like [dubbo-xxx] fix unknownexception when host config not exist #xxx. each commit in the pull request should have a meaningful subject line and body. write necessary unit-test to verify your logic correction, more mock a little better when cross module dependency exist. if the new feature or significant change is committed, please remember to add sample in dubbo samples project. run mvn clean install -dskiptests=false & mvn clean test-compile failsafe:integration-test to make sure unit-test and integration-test pass. if this contribution is large, please follow the software donation guide. ", merge from dubbo master  merge from dubbo master  remove duplicate code ,,[dubbo-4491]remove duplicate code in abstractconfig.java,merge from dubbo master,remove duplicate code in abstractconfig.java,[dubbo-xxx] remove duplicate code in abstractconfig.java, [dubbo-4500] Remove duplicate code in AbstractConfig for cleaner implementation,remove duplicate code in abstractconfig
" if a diagnostic request is actually canceled, we will throw away the type checker as we cannot be certain that it is still in a usable state. i recommend reviewing this with ?w=1 to make the diff easier to understand. ", make type-checking cancellable.  conflicts:  src/compiler/checker.ts  src/compiler/program.ts  src/compiler/types.ts  src/services/services.ts ,,make it possible to cancel requests to get diagnostics.,make type-checking cancellable.,make type-checking cancellable.,Make type-checking cancellable, make type-checking cancellable,make type-checking cancellable
" this pr adds a new pipeline to ci for testing builds under cuda 11.0. the new pipeline (""unix-gpu-cu110"") is triggered by the full-build when the sanity build completes. ", add new docker containers for cuda 11.0 and libcudnn8.  add new functions for running gpu builds and tests in new cuda11 containers.  add runtime functions for cuda 11.0 related builds/tests.  add new pipeline for testing cuda 11.0 builds. ,,add new ci pipeline for building and testing with cuda 11.0.,add new pipeline for testing builds under cuda 11.0,add new pipeline for testing cuda 11.0 builds,add new pipeline for testing cuda 11.0 builds, Add new CI pipeline for testing CUDA 11.0 builds,add new pipeline for testing cuda 11.0 builds
" bug fixes apis added type and value checks for parameter ""shape"" in static graph, when passing ""shape"" to function ""fill_constant()"". bugs are described as below: "," fixed bugs  fixed bugs  some modifications have been made in branch junhui_dev, now i have to update from upstream develop before push ",,fixed bugs for 2.0 api,"fix apis for ""shape"" in static graph",bug fixes for parameter shape in static graph,fix bugs in static graph when passing shape to fill_constant," fix parameter checks for ""shape"" in static graph - bug fixes in fill_constant() function",fixed bugs for shape parameter in fill_constant
" noticed that syntax like vec![0; 5] is never mentioned in vec<t>'s docs, nor used in any of its methods' docs, so i figured i should add a mention of it. also noticed vec!(1, 2) being used in one spot while i was at it, so i fixed that as well for consistency's sake. r? @steveklabnik ", mention vec![x; len] syntax in vec docs  make docs for vec::push() use vec! with square brackets ,,"mention vec![x; len] syntax in vec<t> docs, fix inconsistent use",use vec! with square brackets in vec docs,make docs for vec::push() use vec! with square brackets,fix vec documentation to mention vec![x; len] syntax, add vec![x; len] syntax to vec docs and fix vec! usage consistency,mention vec![] syntax in vec's docs
" modified app.now to correctly return the  timezone-aware current date/time, according to the configured timezone or utc. added unit tests for the changed/added methods. optimize tests by reducing wait time for some test cases due to default polling interval value. fixes #3753. "," fix wrong task eta when using timezone setting  optimization: reduce polling interval in test cases  test cases for app.now, app.uses_utc_timezone ",,fix task eta issues when timezone is defined in configuration,optimize timezone setting for app.now,fix wrong task eta when using timezone setting,fix timezone-aware current date/time in app.now, Fix Timezone Handling in Datetime Functions and Enhance Test Dependencies,fix wrong eta calculation when using timezone
" similar to #65353 (which this pr should've been a part of), however in this case we didn't previously nest the tables when processing trait paths in impl block declarations. closes #65411 ", save-analysis: nest tables when processing impl items  save-analysis: add a relevant test case , -z save-analysis ice when processing impl under an fn item ,nest tables when processing impl block definitions,nest tables when processing impl items,nost tables when processing trait paths in impl block declarations,fix ice when resolving trait paths in impl block declarations, fix ICE when processing trait paths in impl block declarations (#65411),save-analysis: fix ice when processing impl under an fn item
 since esp32 stage has no core_version.h file we have to disable the include. this is done with setting esp32_stage=true in platformio changed esp32 stage to latest commit f7fb00632e0.... the pull request is done against the latest dev branch only one feature/fix was added per pr. the code change is tested and works on tasmota core esp8266 v.2.7.4.5 the code change is tested and works on core esp32 v.1.12.2 i accept the cla. note: the code change must pass ci tests. your pr cannot be merged unless tests pass , no core_version.h in esp32 stage  build flag for esp32 stage ,,make esp32 stage compile possible,build flag for esp32 stage,disable include in esp32 stage,fix esp32 stage include issue, disable core_version.h inclusion for esp32 stage,disable core_version.h for esp32 stage
 backport #13603 to 3-0-x. ," add api to return an unique id for page  fix double-freeing remote references  after the page does navigations, garbage collection can still happen in  the old context. this commit changes to store references to remote objects  by _pages_, instead of by _webcontents_. ",,guard against double-freeing remote references (3-0-x),backport #13603 to 3-0-x,add api to return a unique id for page,backport pin lineage of plasma objects that are still in scope to 3-0-x, backport #13603 to 3-0-x: fix double-freeing remote references and add unique page ID API,
 kafka-12541 introduced a regression for listoffsets requests for non maxtimestamp specs. when communicating with old brokers. this pr addresss this case. tested with new unit test for regression case. , kafka-12541 add max_timestamp spec to listoffsets api  kafka-12541 updated replica fetcher to use latest listoffsets request version  kafka-12541 refactor with retry approach in kafkaadminclient  kafka-12541 added logoffsettest tests  kafka-12541 tidy up  kafka-12541 fixes per pr review  kafka-12541 adminclient simplification  kafka-12541 refactor of listoffsets retry per pr review comments  kafka-12541 fixes per pr review  kafka-12541 stopped retries for partitions that cannot use max_timestamp  kafka-12541 roll back changes to requestresponsetest  kafka-12541 fixes per pr review  kafka-12541 fixes per pr review  kafka-12541 fixes per pr review  kafka-13002 fix for immediate downgrade cases for non max timestamp requests ,,listoffsets must downgrade immediately for non max_timestamp specs,add max_timestamp spec to listoffsets api,kafka-12541 add max_timestamp spec to listoffsets api,Fix regression for listoffsets requests with non-maxtimestamp specs, Fix Regression in ListOffsets for Non-MaxTimestamp Specs (Kafka-12541),kafka-12541: handle listoffsets requests for non-max timestamp specs
" this work does the following: removes the banner add for typeform-based font awesome survey reinstates all other fonticons/black tie banner ads changes references in code and content from ""fonticons"" to ""fort awesome"" updates color combinations for fort awesome (using fa's off-black) and black tie banner ads note: the survey page ( reviewers ui, fed, and content - @davegandy ", removing survey promotion to homepage  updating banner ads visually and name-wise to reference fort awesome  adjusting color settings for black tie banner ad ,,remove font awesome feedback survey,updating banner ads to homepage,remove banner add for typeform-based font awesome survey,remove banner add for typeform-based font awesome survey and update references, update banner ads and color settings for fort awesome,update homepage to remove font awesome survey ad and update banner ads to reference fort awesome
 fixes #33197 , removing duplicate text-decoration style for abbr[title] #33197  removing duplicate text-decoration style for abbr[title] #33197  fix: removing duplicate text-decoration style for abbr[title] #33197 , bootstrap reboot 4.x css contains duplicate text-decoration style for abbr[title] ,remove duplicate text-decoration style for abbr[title],removing duplicate text-decoration style for abbr[title],bootstrap reboot 4.x css contains duplicate text-decoration style,fix duplicate text-decoration style for abbr[title], Remove duplicate text-decoration style for abbr[title] in Bootstrap 4.x,remove duplicate text-decoration style for abbr[title] #33197
" add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). provide a url to documentation or source code which provides context for the suggested changes:  include tests for your changes if you are making substantial changes, consider adding a tslint.json containing { ""extends"": ""dtslint/dt.json"" }. if for reason the any rule need to be disabled, disable it for that line using // tslint:disable-next-line [rulename] and not for whole package so that the need for disabling can be reviewed. ", removing prompt verbage from selection binding options  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  merge remote-tracking branch 'upstream/master'  adding a link to the search options article ,,adding a link to the word search options details page,adding a link to the search options article,removing prompt verbage from selection binding options,updating office-js-preview with the latest from the cdn, Adding Link to Search Options Article Documentation,update the search options article with a link
" this pr adds support for the decrqss (request selection or setting) escape sequence, which is a standard vt query for reporting the state of various control functions. this initial implementation only supports queries for the decstbm margins, and the sgr graphic rendition attributes. this can be useful for certain forms of capability detection (#1040). as one example in particular, it can serve as an alternative to the colorterm environment variable for detecting truecolor support (#11057). of the settings that can be queried by decrqss, the only other one that we could be supporting at the moment is decscusr (cursor style). however, that would require passing the query through to the conpty client, which is a lot more complicated, so i thought it best to leave for a future pr. for now this gets the basic framework in place, so we are at least responding to queries, and even just supporting the sgr attributes query is useful in itself. i've added a unit test verifying the reports for the decstbm and sgr settings with a range of different parameters. i've also tested the decstbm and sgr reports manually in vttest, under menu 11.2.5.3.6 (status-string reports). ", add the basic framework for decrqss.  add a handler for the decstbm setting.  add a handler for the sgr setting.  add some unit tests.  appease the spelling bot. ,,add basic support for the decrqss settings query,add support for decrqss (request selection or setting) escape sequence,add support for decrqss escape sequence,add support for the DECRAQSS escape sequence, Add support for the DEC RQSS escape sequence for querying terminal settings,add support for decrqss queries
 trigger the change event on the next tick. this means that multiple changes to a track's mode will only result in a single change event on its associated texttracklist rather than 3 events as it may be currently. fixes #5159. , add helper to event target to queue events  queuetrigger change events  lint error ,,async change events in texttracklist with eventtarget#queuetrigger,trigger change event on the next tick,add event target to queue events,add support for batching change events on TextTrackList, Queue change events on next tick to reduce event count (#5159),queue change events on text tracks (#5159)
" change types to have latest api in  p.s: current readme wasn't updated after keys() method removal, pr already exist to fix that: jonschlinkert/parse-git-config#11 and there is only expandkeys() method left: ", sync latest parse-git-config api changes  parse-git-config - fix tabulation ,,sync with latest api changes,sync latest api with latest api,sync latest parse-git-config api changes,update parse-git-config API to latest keys() method removal, updated parse-git-config api with latest changes,sync latest parse-git-config api changes
 new features apis add iterabledataset support for multiprocess dataloader add paddle.io.iterabledataset base class add paddle.io.get_worker_info to get worker process information for data splitting in iterabledataset , add iterabledataset support in multiprocess dataloader. test=develop  fix single process exit. test=develop ,,add iterable dataset support for multiprocess dataloader,add iterabledataset support for multiprocess dataloader,add iterabledataset support for multiprocess dataloader,add iterabledataset support for multiprocess dataloader, add iterabledataset support for multiprocess dataloader in paddle,add iterabledataset support for multiprocess dataloader
 document manage_inactivity function allow extruder_runout_prevent to work with all extruders use faster memcpy for copying coordinates fix some spelling in the configurations , spacing and spelling  fix manage_inactivity  - document manage_inactivity function  - allow extruder_runout_prevent to work with all extruders  - use faster memcpy for copying coordinates ,,"optimize coordinate copying, fix extruder_runout_prevent",allow extruder_runout_prevent to work with all extruders,document manage_inactivity and use faster memcpy,implemented support for kill_pin / fixed compilation errors for incomplete/bad translations," documented manage_inactivity, enhanced extruder_runout_prevent, and optimized memcpy usage",various fixes
 adds unit and integration tests for updating the backend config. this also standardizes the old create_backend codepath and the new deploy codepath on the same underlying call to update a backend config. i've run scripts/format.sh to lint the changes in this pr. ," unit tests for backend state  add goals  add explicit goal tests  remove  fix non-detached  small fix  fix version  add cleanup(), include in unit tests  fixes  add exit_actor call  medium for test_handle  refactored unit tests  add tests  add unit tests for updating config ",,add backend_state tests for updating backend config,add unit tests for updating backend config,add unit tests for updating backend config,add unit and integration tests for updating the backend config, add unit and integration tests for backend config updates,add unit and integration tests for updating backend config
" fix: #9401 note: i added some extra code that checks the operand values which is not strictly necessary, but now the errors are exactly the same as when using mongo shell. ", add support for  aliasses  styling ,,add support for $type aliasses.,add support for aliasses styling,add support for aliasses in mongo shell,fix: support for aliasses, fix operand value checks for consistency with mongo shell (fixes #9401),fix: mongoose schema type error when using $where with string operands
" most network params in dask module is constructed manually based on ips and ports. we need to drop all aliases from params to not confuse lightgbm cpp code with multiple values for the same param. lightgbm/python-package/lightgbm/dask.py lines 125 to 135 in ac706e1 machine_list = ','.join([ '%s:%d' % (urlparse(worker_address).hostname, port) for worker_address, port in worker_address_to_port.items() ]) network_params = { 'machines': machine_list, 'local_listen_port': worker_address_to_port[local_worker_address], 'time_out': time_out, 'num_machines': len(worker_address_to_port) } ", update dask.py  update basic.py ,,drop aliases of core network parameters,ac706e1 line 125 to 135 in dask module,drop all aliases from network params,Refine network params in Dask module for LightGBM compatibility, remove aliases from network params in dask module,drop aliases in dask params
 i hereby agree to the terms of the cla available at:  for changelog. remove if this is non-significant change. , docapi-5203: direct i/o settings for mergetree descriptions.  docapi-5203: edits after prereview of pull. ,,direct i/o settings for mergetree descriptions. en review and ru translation.,docapi-5203: direct i/o settings for mergetree descriptions.,direct i/o settings for mergetree descriptions.,docapi-5203 direct i/o settings for mergetree descriptions, docapi-5203: direct i/o settings for mergetree descriptions and prereview edits,docapi-5203 direct i/o settings descriptions.
" it turns out that in rare cases, ensureresources can fail. it's not currently known what can cause it. but the outcome of when it fails is that we hit an error accessing properties on an undefined object.  so this fix at least throws an error in render and doesn't try to render the app causing misleading errors. addresses #19959 ", fix(gatsby): ensure that ensureresources ensures resources  test ,,improve error message when ensureresources fails to ensure a resource,ensureresources ensures resources test,ensure that ensureresources ensures resources,Fix ensureresources to handle rare failure cases, Fix ensureResources failure to prevent undefined object property access,gatsby: throw an error when ensureresources fails instead of trying to render app
" nothing has been turned on yet. an incremental build involving incremental external dependencies behaves as a hybrid between an external dependency and a normal swiftdeps-laden swift file. in the simplest case, we will fall back to the behavior of a plain external dependency today. that is, we will check its timestamp, then schedule all jobs that involve these external dependencies if it is out of date. where things get interesting is when cross-module incremental builds are enabled. in such a case, we know that a previous compiler has already emitted serialized swiftdeps information inside of a swiftmodule file. moreover, we know that that swiftmodule file was loaded by the build of the current swift module. finally, thanks to the previous stack of commits, we now know exactly how to extract this information from the swiftmodule file. to bring this all home, we unpack incremental dependency information from external dependencies, then integrate them into the current dependency graph - as though they were any other swiftdeps file. this neatly extends the single-module incremental logic to the multi-module case. "," teach the driver to read fine-grained dependency graphs in swiftdeps files  install incremental external dependency integration code  an incremental build involving incremental external dependencies behaves as a hybrid between an external dependency and a normal swiftdeps-laden swift file.  in the simplest case, we will fall back to the behavior of a plain external dependency today. that is, we will check its timestamp, then schedule all jobs that involve these external dependencies if it is out of date.  where things get interesting is when cross-module incremental builds are enabled. in such a case, we know that a previous compiler has already emitted serialized swiftdeps information inside of a swiftmodule file. moreover, we know that that swiftmodule file was loaded by the build of the current swift module. finally, thanks to the previous stack of commits, we now know exactly how to extract this information from the swiftmodule file. to bring this all home, we unpack incremental dependency information from external dependencies, then integrate them into the current dependency graph - as though they were any other swiftdeps file. this neatly extends the single-module incremental logic to the multi-module case. ",,teach the legacy driver to unpack incremental dependency information from swiftmodule files,incremental build involving incremental external dependencies,install incremental external dependency integration code,serialization for incremental external dependencies, Integrate incremental external dependencies into multi-module builds,incremental external dependencies
" fixes a part of #9325 fixes docstring parameters test for cross_decomposition and discriminant_analysis the issue is not closed, other packages still need to be checked. don't hesitate to tell me if i can improve anything in this pr! ", fixing cross_decomposition docstring parameters  fixing discriminant analysis docstring parameters ,,docstring parameters improvements for cross_decomposition and discriminant_analysis,fix docstring parameters test for cross_decomposition and discriminant_analysis,fixing docstring parameters for cross_decomposition and discriminant_analysis,fixes docstring parameters for cross_decomposition and discriminant_analysis, fix docstring parameters for cross_decomposition and discriminant_analysis,docstring checks for svm and covariance modules
" per the discussion on the issue tracker, the behavior of the pure python implementation of datetime, date and time are out of whack with the equivalent from _datetimemodule.c. since the c version is almost certainly what's being used almost everywhere, this shouldn't have any real behavioral changes. the equivalent bug in pypy3 has been fixed for some time. ", add failing test for pure python datetime subclasses  bring pure python (datetime|date|time).replace behavior in line with c ,,make (datetime|date|time).replace return subclass type in pure python,bring pure python (datetime|date|time) behavior in line with c,bring pure python (datetime|date|time).replace behavior with c,"fix datetime, date, and time pure python implementation to match _datetimemodule.c behavior", align pure python datetime behavior with C implementation,bring pure python datetime/date/time behavior in line with c
 docs(vi-mode): revamp readme and document settings fix(vi-mode): hide cursor-change logic behind vi_mode_set_cursor setting fixes #9570 , docs(vi-mode): revamp readme and document settings  fix(vi-mode): hide cursor-change logic behind vi_mode_set_cursor setting  fixes #9570 , vi-mode changed the cursor shape; how do i disable it? ,hide cursor-change logic behind opt-in setting,docs(vi-mode): revamp readme and document settings,revamp readme and document settings,fix vi-mode settings," Revamp README and document vi-mode settings, fix cursor-change logic",hide cursor-change logic behind vi_mode_set_cursor setting
" backport of #17538. npm test passes tests are changed or added pr title follows semantic commit guidelines pr release notes describe the change in a way relevant to app developers, and are capitalized, punctuated, and past tense. notes: fixed offscreen rendering not working with viz compositor. ", fix: make osr work with viz compositor  fix: update osr patch  fix: update patch again  fix: update viz_osr.patch for macos  fix: gn check warnings  chore: no need to change softwareoutputdevicewinproxy  chore: add check in case we missed something  fix: consider scale factor when compare size  fix: make gpu osr work  fix: autofill popups with osr  chore: use unix line ending for osr_video_consumer  chore: code is already in defined(os_macosx)  fix: share same osr implementation on macos  this should also fix the crash when there is navigation on macos.  test: osr window should not crash after navigation  fix: make osr work on mac properly  fix: software osr on windows  fix: software osr on linux  fix: split local surface id allocation into two  fix: update patch for 5-0-x  fix: patch and update videoconsumer to report proper damage_rect ,,port osr code to new viz compositor codepath (backport: 5-0-x),fix offscreen rendering not working with viz compositor,make osr work with viz compositor (backport),Make osr work with viz compositor, fix offscreen rendering with viz compositor (backport: 5-0-x),make osr work with viz compositor (backport: 5-0-x)
" new module pull request network/aos/aos_device ansible version ansible 2.3.0 (aos_device_clean de9d90c9e6) last updated 2017/02/10 15:59:02 (gmt -700) config file = configured module search path = default w/o overrides i'm working for apstra, we are making a product to automate datacenter networks and we are developed a dozen of ansible modules and one dynamic inventory to control our product with ansible. we have tried to follow ansible's best practices as much as possible, all our modules are idempotent and support the mode --check. currently, this module has limited features but the few it has are very useful :). we have implemented the parameter state even if it only supports 1 state right because we know that we will add more very soon, unfortunately it won't be ready for the 2.3 deadline. this is the third batch of modules that i'm submitting for review, the first aos_ip_pool #21044 has been recently approved. ", initial version of aos_device  clean up documentation  move try/except closer to device.approve  remove non valid characters ,,aos_device as part of network/aos,new module pull request network/aos/aos_device ansible version 2.3.0,aos_device - first batch of modules,add support for apstra's aos_device module, add aos_device module for apstra datacenter automation,add aos_device module for apstra
 this pr fixes the problem that in some server-side sdks no user data is attached to session data and so no users adoption metric information it solves this by calculating adoption based on sessions in addition to users adoption calculations , added tests for calculating release adoption in sessions mode  added logic that allows adoption to be calculated either based on sessions or users  modified adoption calculation to return both users and sessions adoptoin  added sessions adoption to adoption calculation tests ,,add session adoption to release-health,fix adoption in sessions mode,fix release adoption metric in sdks,add support for session adoption, Add support for session-based adoption calculation,add support for calculating adoption based on sessions
 db2foreignkeyconfigurator db2indexconfigurator db2schemaconfigurator db2sequenceconfigurator db2uniquekeyconfigurator in org.jkiss.dbeaver.ext.db2.ui based on org.jkiss.dbeaver.ext.db2 managers of the same name created , #8562 part (configurators) of ui-part from plugins/org.jkiss.dbeaver.ext.db2 moved in org.jkiss.dbeaver.ext.db2.ui  #8562 wrong path in plugin.xml fixed ,,#8562 configurators from ext.db2 moved in ext.db2.ui,db2foreignkeyconfigurator db2indexconfigurator db2uniquekeyconfigurator,#8562 part (configurators) of ui-part moved in org.jkiss.dbeaver.ext.db2.ui,db2 configurators moved to ui, #8562 db2 configurators moved to org.jkiss.dbeaver.ext.db2.ui,#8562 db2 ui-part moved to ext.db2.ui
" ref: kubernetes/org#2456 as a part of cleaning up inactive members (those with no activity within the past 18 months), this commit removes the following people from various owners files: @balajismaniam @gmarek @jianhuiz @madhusudancs @mbohlool @nikhiljindal @csbell this pr replaces #99073, #99075 and #99077 / /assign @liggitt ", *: move balajismaniam to emeritus_approvers  *: move gmarek to emeritus_approvers  *: remove jianhuiz from reviewers  *: remove madhusudancs from reviewers  owners_aliases: remove csbell from feature-approvers  *: remove mbohlool from reviewers  *: remove nikhiljindal from owners ,,remove inactive members from owners - jan 2021,remove nikhiljindal from owners files,remove inactive members from feature-approvers,clean up inactive members from owners files, remove inactive members from owners files across directories,remove inactive members from owners
" use public paths in module augmentation test documented module augmentation / we have a notion of what we consider ""public paths"". however, this does not apply to type imports which makes it harder to teach (""ok here, but not ok there""). since we already make types public by documenting that the paths are public we might as well export them directly which means we can simplify how we teach public paths. the existing patterns continue to work but are more likely to break during refactoring. the patterns we now document are tested in ci and therefore a lot more robust. implementation choices separate script does not slow down test:unit allows parallel testing (which we haven't set up for mocha) can be included in test suite that runs with typescript nightlies ", [core] simplify module augmentation  test in ci ,,support public paths in module augmentation,simplify module augmentation test in ci,use public paths in module augmentation test,Simplify module augmentation and use public paths, Simplify and Test Module Augmentation in CI,simplify module augmentation test patterns
" another step toward #11638 this pr is a continuation of #12732. there were blas calls in the following cython files: weight_vector.pyx, cd_fast.pyx and    _k_means.pyx. there were also blas calls in the c file cholesky_delete.h. i decided to move (using fused types) the only function there to cython in arrayfuncs.pyx where there was a python wrapper for it. i added rot an rotg to the cython blas functions which i forgot in #12732 because i didn't check c files... i also removed linking to blas libs in some setups for modules which didn't call blas functions. the only remaining calls to bundled cblas in sklearn appear in tron.cpp, used in liblinear. they can't be replaced easily by calls to scipy cython blas because the calls to blas are not from cython files but from c files. ", replace cblas calls in utils/weight_vector  add rot & rotg to cython_blas  cholesky_delete -> cython to use cython_blas  add rot & rotg to cython_blas  rot & rotg cython_blas tests  cholesky delete fix  cblas to scipy cython_blas in linear_model/cd_fast  cblas to scipy cython_blas in cluster/_k_means  cleanup manifold setup ,,continue moving from cblas to scipy cython blas,replace cython blas calls in c files,replace cblas calls in cython files,remove blas calls from cython files and add to cython_blas, replace cblas calls with scipy cython_blas in sklearn and cleanup setup,remove cblas linking in some setups
" consolidating some bug fixes and improvements to the world api. work in progress. handle the corner case when ue deletes an actor visually and from the actor list but reference still exists in memory, causing fatal errors when another object of the same name is spawned. queue up garbage collection when an actor is destroyed. use a tmap for name<->asset mapping, instead of iterating through all assets in the database in getmeshfromregistry at every spawn. use a tmap for name<->actor mapping, instead of iterating through all actors in scene at every object api call. ", use a tmap for asset registry  handle ue's garbage collection delays when spawning. don't force particular names  print message when asset registry is ready ,,fixes and improvements to world api,fix ue's garbage collection delays when spawning,improvements to the world api,consolidating bug fixes and improvements to the world api, consolidate world api improvements and bug fixes,consolidate world api fixes
" fixes #87718 the problem was that synth_type_param_count was already subtracted from named_type_param_count, so this ended up being subtracted again. this caused expected_min to overflow, and ultimately resulting in weird and wrong behaviour. i've also added another test not present in the original issue but caused by the same bug. ", fix overflow when calculating expected_min in generics diagnostics  add regression tests ," ice: 'assertion failed: num_missing_args > 0', compiler/rustc_typeck/src/structured_errors/wrong_number_of_generic_args.rs:234:17 ",explicit_generic_args_with_impl_trait: fix min expected number of generics,fix overflow when calculating expected_min in generics diagnostics,fix overflow when calculating expected_min in generics diagnostics,fix overflow in generics diagnostics, fix overflow in generics diagnostics and add regression tests,fix overflow calculating expected_min in generics diagnostics
 description: an exception is thrown to the user if a custom resolver name is specified when using strict or logical dns in the address section of the endpoints. risk level: low testing: //test/... docs changes: this is a behavior change in an error condition. would documenting this change in the proto file be sufficient? release notes: n/a fixes #3553 , merge latest master from envoy  merge envoy master  throw exception if custom resolver is specified with strict_dns or logical_dns , prevent the use of custom resolvers for dns discovery types ,disallow specifying custom resolver name for strict and logical dns,envoy master throw exception if custom resolver is specified with strict_dns or logical_dns,throw exception if custom resolver name is specified when using strict or logical dns,prevent the use of custom resolvers for dns discovery types, Throw exception if custom resolver is specified with strict_dns or logical_dns,throw exception if using custom resolver with strict_dns or logical_dns
 renamed outdep to out_dep. added a simple test for .d files generated from -mmd. ," fixed dependency files not being saved away if using -mmd or similar.  fixed timestamps on object files extracted from archives during link.  this affects builds that use absolute paths to object files when adding to library archives, causing unnecessary archives.  renamed outdep to out_dep.  added test for change for issue #1732. ",,further changes for issue #1732,renamed outdep to out_dep.,fixed dependency files not being saved away if using -mmd or similar.,fixed dependency files and timestamps in out_dep, fixed dependency file handling and added test for issue #1732,fixed dependency files not being saved away if using -mmd or similar
" using the gi command provided by the gitignore plugin was causing a % to be shown in the terminal at the end of output. i opened an issue on the gitignore.io repository and they suggested adding -w '\n' to the curl command in order to make curl add the new line character at the end. the linked issue explains what my problem was in more detail. this pr applies the suggested fix. further, arguments expansion has been double-quoted for security reasons. see "," add trailing new line at the end of output  double-quote variable expansion  it's good practice to double-quote variable expansions, for security  reason.  see ",,minor improvements to the gitignore plugin,double-quote variable expansions for security reasons,add trailing new line at the end of output,use command to ensure trailing newline at the end of output, fix gitignore plugin output formatting and enhance security,add trailing new line at the end of output
" add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). the package does not already provide its own types, or cannot have its .d.ts files generated via --declaration create it with dts-gen --dt, not by basing it on an existing project. represents shape of module/library correctly tslint.json should be present and it shouldn't have any additional or disabling of rules. just content as { ""extends"": ""dtslint/dt.json"" }. if for reason the some rule need to be disabled, disable it for that line using // tslint:disable-next-line [rulename]  and not for whole package so that the need for disabling can be reviewed. tsconfig.json should have noimplicitany, noimplicitthis, strictnullchecks, and strictfunctiontypes set to true. provide a url to documentation or source code which provides context for the suggested changes: < if you are making substantial changes, consider adding a tslint.json containing { ""extends"": ""dtslint/dt.json"" }. if for reason the any rule need to be disabled, disable it for that line using // tslint:disable-next-line [rulename] and not for whole package so that the need for disabling can be reviewed. if a package was never on definitelytyped, you don't need to do anything. (if you wrote a package and provided types, you don't need to register it with us.) add it to notneededpackages.json. "," added chatpermissions interface  - the field all_members_are_administrators has been removed from the documentation for the chat object. the field is still returned in the object for backward compatibility, but new bots should use the permissions field instead.  - modified chat interface. added : permissions, can_set_sticker_set, and sticker_set_name  updated message interface & added new interfaces - poll & polloption  - forward_sender_name  - media_group_id  - caption_entities  - poll  - connected_website  modified exisiting interfaces - audio, animation  - removed redundancy by extending filebase on animation  - added new properties to animation : width, height, and duration  - added new property to audio : thumb  modified contact and venue interfaces  - added foursquare_type to venue  - added vcard to contact  added new interface loginurl  rearranged interface ordering so that it matches telegram documentation  updated chatmember interface  - rearranged ordering to match telegram documentation  - added new property : is_member  fixed linting errors and made login_url as optional param in inlinekeyboardbutton ",,added new interfaces and updated existing ones,added chatpermissions interface - rearranged interface ordering,"added new interfaces - chatpermissions, contact, and venue",Added types for Telegram API interfaces and updated tests, Updated Telegram Bot API Types and Interfaces,Telegram Bot API Types: Merged PRs
" i just extended the types of the arguments for the #registerpartial function, because per the implementation that function is polymorphic. if the first argument is a string then the second must be a template fn, but in case the first argument is an object then the second argument is discarded making it optional. the implementation in question "," updating types for #registerpartial  #registerpartial is a polymorphic function now it accepts a single object making the second param optional.  excerpt from the implementation:  js  registerpartial: function(name, partial) {  if (tostring.call(name) === objecttype) {  extend(this.partials, name);  } else {  if (typeof partial === 'undefined') {  throw new exception(attempting to register a partial called ""${name}"" as undefined);  }  this.partials[name] = partial;  }  },    updated handelbars#registerpartials  removed extra spaces ",,updated type definitions for handlebars#registerpartials,updating types for #registerpartial,updating types for #registerpartial,Update types for #registerpartial, Update types for #registerpartial to handle polymorphic behavior,update types for #registerpartial
" fixes #4817. when the --update-checksums flag is set, yarn would know to ignore a checksum mismatch between yarn.lock and the repository, and instead update the yarn.lock file with the proper checksum(s). added new tests. to manually check this: change one or more of the package checksums in yarn.lock delete node_modules (optionally also run yarn cache clean) run yarn => checksum mismatch error will be received. run yarn --update-checksums => will install successfully and fix the damaged checksums in yarn.lock ", test(cli): update package checksum  update package checksum when there is a checksum mismatch between the repo and the lockfile  feat(cli): update package checksum on mismatch  provide the option to update the package checksum when there is a mismatch between the repo and the  local yarn.lock file  4817 , moving repositories without recreating yarn.lock? ,add --update-checksums to cli install,fix the re-installing of the re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re-installed re,update package checksums when there is a mismatch between yarn.lock and the repository,fix yarn to update package checksums when --update-checksums flag is set, add --update-checksums flag to handle yarn.lock checksum mismatches,feat(cli): update package checksum on mismatch
" upon further coding proceeds, found another unsquashed macos unavailable nproc. decreases travis build time too! fix last pr that uses the wrong command of phy cores instead of logical cores. "," nproc for ubuntu, hw.logicalcpu for mac  physicalcpu to logicalcpu  non-efficient j1 to hw.logicalcpu ",,changes nproc for mac to hw.logicalcpu where applicable,"fix nproc for ubuntu, hw.logicalcpu for mac physicalcpu to logicalcpu non-efficient","nproc for ubuntu, hw.logicalcpu for mac",Fix command consistency checks and error fixing in mac build instructions, Improve macOS build efficiency and correct CPU core count logic,get cpu info for macos
" description: added support for humidity and pressure in 1-wire devices. also added support for a couple of new (old) devices. this update changes the names of the sensors from ""<sensor_name>"" to ""<sensor_name> <sensor_type>"" example: kitchen -> kitchen temperature. in the database this looks like: sensor.kitchen -> sensor.kitchen_temperature. this was a necessary change as devices with multiple sensors would be given additional _n suffixes in the database that would be not be persistent per sensor across restarts of ha. if you wish to maintain a single line of record in the database this can be achieved by the following recipe. connect to your database using the instructions from  check the names of sensors: select entity_id, count(*) as count from states group by entity_id order by count desc limit 10; alter the names of sensors using the following examples: update states set entity_id='sensor.<sensor_name>_temperature' where entity_id like 'sensor.<sensor_name>%' and attributes like '%\u00b0c%'; update states set entity_id='sensor.<sensor_name>_pressure' where entity_id like 'sensor.<sensor_name>%' and attributes like '%mb%'; update states set entity_id='sensor.<sensor_name>_humidity' where entity_id like 'sensor.<sensor_name>%' and attributes like '%%%' escape ''; remember to replace <sensor_name> with the actual name of the sensor as seen in the select query. checklist: local tests with tox run successfully. your pr cannot be merged unless tests pass new dependencies have been added to the requirements variable (example). new dependencies are only imported inside functions that use them (example). new dependencies have been added to requirements_all.txt by running script/gen_requirements_all.py. new files were added to .coveragerc. ", added more devices and sensor types.  flake8 fixes ,,added more devices and types to onewire,add support for humidity and pressure in 1-wire devices,added support for humidity and pressure,add support for humidity and pressure in 1-wire devices, Add support for humidity and pressure in 1-wire devices,add support for 1-wire humidity and pressure sensors
" add support for group layers when parsing json output from tiled as requested in issue #4099.  along the way, i believe i discovered some problems with the way phaser parses infinite tiled maps and fixed those as well.  infinite map layers might still have 'offsetx' and 'offsety' properties but these were previously being ignored.  they are now accounted for properly. i have tested all these changes against the phaser3-examples both visually and with the 'tester' script and everything that worked before the changes works the same as after my changes.  i added several tests to the examples repo in my local working copy that tested groups and an infinite map to ensure those parts work as well.  those examples are not a part of this pr as they were on top of the examples repo and not this one. here is a quick summary of changes: added logic to each of the layer type parsers (tiled, object, and image) to account for group layers. added a file with a single function that creates the important state that needs to be inherited from a group. uses an iterative approach to traversing the group hierarchy with a stack rather than recursion. all hierarchy is flattened to a single array of layers and the group layers are discarded after they are accounted for. layer names are prepended with any containing group names separated by slashes ('/') updated documentation for the tilemap gameobject to mention support for layers and the renaming that can occur. updated error reporting when an invalid tile layer name is requested in order to pre-empt any confusion surround the renaming of layers.  now lists the valid layer names. "," support for tiled groups and infinite map fixes  - added support for tiled group layers (issue #4099)  - fixed some layer offset bugs for infinite maps  updates for supporting group layers  - updated documentation to mention support for groups and naming layers  - added more verbose output when an unknown layer name is specified  > error output now lists the valid layer names  - added functions to return array of tile, object, or image layer names ",,support for tiled layer groups,support for infinite maps and tiled groups,support for tiled groups and infinite map fixes,python net spec cleanups and top-less layers, add support for group layers in tiled json parsing and infinite map fixes,add support for tiled groups and infinite maps
" closes #19950 tests added / passed passes git diff upstream/master -u -- ""*.py"" | flake8 --diff whatsnew entry very simple implementation at the moment. the thought here is to introduce this method and perhaps subsequently extend to allow for string concatenation of the elements. longer term there could also be a keyword added to .agg of groupby which will dispatch to this instead of simply returning a multiindex column, which could alleviate some of the pain users are experience when trying to rename columns after an aggregation. @tomaugspurger and @jorisvandenbossche from the dev chat today ", added initial test  method implementation , add method to flatten all multi-index levels ,add to_flat_index method to multiindex,add method to flatten all multi-index levels,add method to flatten all multi-index levels,added method to flatten all multi-index levels, add method to flatten multi-index levels,add method to flatten all multi-index levels
 i hereby agree to the terms of the cla available at:  category: doc fixes ('ru'). changes description: adding description of the check table query. ordering miscellaneous queries by alphabet. , translation for check table query to russian  fix link to nowhere ,,adding description of the check table query to the 'ru' doc,translation for check table query to russian,translation of check table query to russian,added description of check table query and fixed links to nowhere, Add check table query description and alphabetize queries in Russian docs,adding description of the check table query. ordering miscellaneous queries by alphabet.
" if the flutter_tool's .packages is missing, run pub get offline in the directory. makes #41681 a bit better, though it still crashes on the first run. ", workaround for cache issue  fix imports ,,handle missing .packages file in the flutter tool for prebuilt artifacts,fix imports in flutter_tool,fix cache issue in flutter_tool,fix cache issue with missing.packages file, workaround for missing .packages in flutter tools,workaround for missing global packages cache
" updated stop-only to the version that only finds describe.only, context.only and it.only and does not find "".only"" in comments add npm run warn-only precommit and npm run stop-only prepush ", chore: upgrade stop-only and catch them  run stop-only first on precommit  just warn on precommit .only  testing  testing done  run stop-only on git prepush hook ,,update stop only and catch em,"update stop-only to the version that only finds describe.only, context.only and it.only",upgrade stop-only to the version that only finds describe.only and context.only,"update stop-only to only find describe.only, context.only and it.only", upgrade stop-only and add precommit/prepush hooks,"stop-only and warn-only for describe.only, context.only and it.only"
" i haven't removed ak::out yet, because i am expecting merge conflicts. (i mean pretty much everything wants to write to stdout at some point.) since we build with -werror, marking it with [[deprecated]] will produce a helpful error message and we can do the renaming of ak::new_out to ak::out in a week or so. "," ak: introduce sourcegenerator::fork().  previously, i abused the copy constructor, this is a lot better.  ak: eradicate the uses of out().  ak: add [[deprecated]] to out(). ",,eradicate remaining calls to ak::out().,add [[deprecated]] to out(),remove the use of out().,rename ak::out to ak::new_out, deprecate ak::out and introduce sourcegenerator::fork().,remove all calls to ak::out
